{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBCygEM5hLEa",
        "outputId": "762008d6-06ab-411f-ca4c-833ce99404f0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision scipy tdqm matplotlib scipy transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeVPd_CFqKVF"
      },
      "source": [
        "# Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayIRMJrwqNR2",
        "outputId": "670ddd38-a11b-4eea-e5b8-80e258280f52"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004v5ZfzxX6u"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikG5UIXCnvh6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJh_Om8QzgEY"
      },
      "outputs": [],
      "source": [
        "def image_normalize(image):\n",
        "    image = image.cpu()\n",
        "    n_channels = image.shape[0]\n",
        "    for channel in range(n_channels):\n",
        "        max_value = torch.max(image[channel])\n",
        "        min_value = torch.min(image[channel])\n",
        "        image[channel] = (image[channel] - min_value) / (max_value - min_value)\n",
        "\n",
        "    image = image.permute(1, 2, 0)\n",
        "\n",
        "    return image\n",
        "\n",
        "def print_image(image):\n",
        "    image = image_normalize(image)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "def print_2images(image1, image2):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(image_normalize(image1))\n",
        "    axes[0].set_title('Image 1')\n",
        "\n",
        "    axes[1].imshow(image_normalize(image2))\n",
        "    axes[1].set_title('Image 2')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_result(result):\n",
        "    for image, noised_image, denoised_image in result:\n",
        "        batch_size = image.shape[0]\n",
        "        for idx in range(batch_size):\n",
        "            print_2images(image[idx], denoised_image[idx])\n",
        "            # print_image(image[idx])\n",
        "            # print_image(noised_image[idx])\n",
        "            # print_image(denoised_image[idx])\n",
        "\n",
        "\n",
        "def print_loss(loss_values):\n",
        "    epochs = list(range(1, len(loss_values) + 1))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, loss_values, 'b-o', label='Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Epoch vs Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV83NB-7-Ueo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHS6FSDSn2hy"
      },
      "source": [
        "# NoiseSchedule\n",
        "- betas, alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq0_qYUenyDg"
      },
      "outputs": [],
      "source": [
        "class NoiseSchedule:\n",
        "\n",
        "    def __init__(self, n_timesteps, beta_start=0.0001, beta_end=0.02, device=device, init_type='linear') -> None:\n",
        "        self._size = n_timesteps\n",
        "        if init_type == 'linear':\n",
        "            self._betas = torch.linspace(beta_start, beta_end, n_timesteps).to(device)\n",
        "        if init_type == 'exponential':\n",
        "            self._betas = torch.from_numpy(np.geomspace(beta_start, beta_end, n_timesteps)).to(device)\n",
        "        self._alphas = self._calculate_alphas()\n",
        "\n",
        "        # print(self._betas)\n",
        "        # print(self._alphas)\n",
        "\n",
        "    def _calculate_alphas(self):\n",
        "        self._alphas = torch.cumprod(1 - self._betas, axis=0)\n",
        "        return self._alphas\n",
        "\n",
        "    def get_beta(self, index):\n",
        "        if index >= self._size:\n",
        "            raise IndexError(\"[get] out of index :\", index, \" / size :\", self._size)\n",
        "        return self._betas[index]\n",
        "\n",
        "    def get_alpha(self, index):\n",
        "        if index >= self._size:\n",
        "            raise IndexError(\"[get] out of index :\", index, \" / size :\", self._size)\n",
        "        return self._alphas[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8PxHOV6n-ZV"
      },
      "source": [
        "# ForwardEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHr0pKLWn5tI"
      },
      "outputs": [],
      "source": [
        "class ForwardEncoder:\n",
        "\n",
        "    def __init__(self, noise_schedule) -> None:\n",
        "        self.noise_schedule = noise_schedule\n",
        "\n",
        "    def noise(self, data, time_step):\n",
        "        # time_step : [B]\n",
        "        # data : [B, 1, 32, 32]\n",
        "\n",
        "        alpha = self.noise_schedule._alphas[time_step]\n",
        "        alpha = alpha.reshape(-1, 1, 1, 1)\n",
        "        # alpha : [B, 1, 1, 1]\n",
        "\n",
        "        epsilon = torch.randn(data.shape).to(device)\n",
        "        # torch.randn ~ N(0, 1)\n",
        "\n",
        "        return torch.sqrt(alpha) * data + torch.sqrt(1 - alpha) * epsilon, epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BIC7rsQ8en-"
      },
      "source": [
        "# ReverseDecoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV7lE5EZzyuy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ReverseDecoder:\n",
        "\n",
        "    def __init__(self, noise_schedule, g) -> None:\n",
        "        self.noise_schedule = noise_schedule\n",
        "        self.g = g\n",
        "\n",
        "    def denoise(self, noise_data, time_step, c=None, w=0):\n",
        "        # noise_data : [B, 1, 32, 32]\n",
        "        # c : [B]\n",
        "        # time_step : INT\n",
        "\n",
        "        batch_size = noise_data.shape[0]\n",
        "        # batch_size : B\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # step : [T - 1, T - 2, .. 2, 1, 0]\n",
        "            for step in range(time_step - 1, -1, -1):\n",
        "\n",
        "                t = torch.full((batch_size, ), step).to(device)\n",
        "                t = t.reshape(-1, 1, 1, 1)\n",
        "                # t : [B, 1, 1, 1]\n",
        "\n",
        "                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "                mu = 1 / torch.sqrt(1 - self.noise_schedule._betas[t]) * (noise_data - (self.noise_schedule._betas[t] / (1 - self.noise_schedule._alphas[t])) * predict_noise)\n",
        "                # mu : [B, 1, 32, 32]\n",
        "\n",
        "                if step == 0:\n",
        "                    # if t == 0, no add noise\n",
        "                    break\n",
        "\n",
        "                epsilon = torch.randn(noise_data.shape).to(device)\n",
        "                # epsilon : [B, 1, 32, 32]\n",
        "\n",
        "                noise_data = mu + torch.sqrt(self.noise_schedule._betas[t]) * epsilon\n",
        "                # noise_data : [B, 1, 32, 32]\n",
        "\n",
        "        return noise_data\n",
        "\n",
        "    def implicit_denoise(self, noise_data, time_step, c=None, w=0, sampling_time_step=10):\n",
        "        # noise_data : [B, 1, 32, 32]\n",
        "        # c : [B]\n",
        "        # time_step : INT\n",
        "\n",
        "        batch_size = noise_data.shape[0]\n",
        "        tau = list(range(0, time_step, time_step // sampling_time_step))\n",
        "        S = len(tau)\n",
        "        # print(tau)\n",
        "\n",
        "        # batch_size : B\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # step : [T - 1, T - 2, .. 2, 1, 0]\n",
        "            for i in range(S - 1, -1, -1):\n",
        "\n",
        "                t = torch.full((batch_size, ), tau[i]).to(device)\n",
        "                t = t.reshape(-1, 1, 1, 1)\n",
        "                alpha_t = self.noise_schedule._alphas[t]\n",
        "\n",
        "                alpha_t_1 = torch.full((batch_size, 1, 1, 1,), 1).to(device)\n",
        "                if i - 1 >= 0:\n",
        "                    t_1 = torch.full((batch_size, ), tau[i - 1]).to(device)\n",
        "                    t_1 = t_1.reshape(-1, 1, 1, 1)\n",
        "                    alpha_t_1 = self.noise_schedule._alphas[t_1]\n",
        "\n",
        "                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "                first = torch.sqrt(alpha_t_1) * ((noise_data - torch.sqrt(1 - alpha_t) * predict_noise) / torch.sqrt(alpha_t))\n",
        "                second = torch.sqrt(1 - alpha_t_1) * predict_noise\n",
        "\n",
        "                noise_data = first + second\n",
        "\n",
        "        return noise_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0pbKzSh82Og"
      },
      "source": [
        "# UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olBnE6_v81Nf"
      },
      "outputs": [],
      "source": [
        "def sinusoidal_embedding(n, d):\n",
        "    # Returns the standard positional embedding\n",
        "    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n",
        "    sin_mask = torch.arange(0, n, 2)\n",
        "\n",
        "    embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
        "    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
        "\n",
        "    return embedding\n",
        "\n",
        "\n",
        "class UNetConv2D(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_batchnorm, n=3, kernel_size=3, stride=1, padding=1):\n",
        "        super(UNetConv2D, self).__init__()\n",
        "        self.n = n\n",
        "        self.ks = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        s = stride\n",
        "        p = padding\n",
        "        if is_batchnorm:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, kernel_size, s, p),\n",
        "                                     nn.BatchNorm2d(out_size),\n",
        "                                     nn.SiLU(inplace=True), )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "        else:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(nn.Conv2d(in_size, out_size, kernel_size, s, p),\n",
        "                                     nn.SiLU(inplace=True), )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_size = out_size\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for i in range(1, self.n + 1):\n",
        "            conv = getattr(self, 'conv%d' % i)\n",
        "            x = conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size, is_deconv=True, is_batchnorm=True):\n",
        "        super(UNetUp, self).__init__()\n",
        "        # self.conv = unetConv2(in_size + (n_concat - 2) * out_size, out_size, False)\n",
        "        self.conv = UNetConv2D(out_size*2, out_size, is_batchnorm)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "    def forward(self, inputs0, *input):\n",
        "        outputs0 = self.up(inputs0)\n",
        "        for i in range(len(input)):\n",
        "            outputs0 = torch.cat([outputs0, input[i]], 1)\n",
        "\n",
        "        return self.conv(outputs0)\n",
        "\n",
        "\n",
        "class UNetTimeEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out) -> None:\n",
        "        super(UNetTimeEmbedding, self).__init__()\n",
        "        self.ln = nn.Linear(dim_in, dim_out)\n",
        "        self.activation = nn.SiLU()\n",
        "        self.ln2 = nn.Linear(dim_out, dim_out)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B = inputs.shape[0]\n",
        "\n",
        "        x = self.ln(inputs)\n",
        "        x = self.activation(x)\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        return x.reshape(B, -1, 1, 1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels = 1,\n",
        "        out_channels = 1,\n",
        "        n_steps = 1000,\n",
        "        time_emb_dim = 256,\n",
        "        n_classes = 10,\n",
        "        class_emb_dim = 64,\n",
        "        channel_scale = 64,\n",
        "        feature_scale = 5,\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNet, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        self.feature_scale = feature_scale\n",
        "\n",
        "        # time embedding\n",
        "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "\n",
        "        # conditional variable embedding\n",
        "        self.class_embed = nn.Embedding(n_classes, class_emb_dim)\n",
        "        self.class_embed.weight.data = sinusoidal_embedding(n_classes, class_emb_dim)\n",
        "        self.class_embed.requires_grad_(False)\n",
        "\n",
        "        # filters = [64, 128, 256, 512, 1024]\n",
        "        filters = [channel_scale * i for i in range(1, 1 + feature_scale)]\n",
        "        # filters = [int(x / self.feature_scale) for x in filters]\n",
        "\n",
        "        # downsampling\n",
        "        self.conv1 = UNetConv2D(self.in_channels, filters[0], self.is_batchnorm)\n",
        "        self.temb1 = UNetTimeEmbedding(time_emb_dim, filters[0])\n",
        "        self.cemb1 = UNetTimeEmbedding(class_emb_dim, filters[0])\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = UNetConv2D(filters[0], filters[1], self.is_batchnorm)\n",
        "        self.temb2 = UNetTimeEmbedding(time_emb_dim, filters[1])\n",
        "        self.cemb2 = UNetTimeEmbedding(class_emb_dim, filters[1])\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = UNetConv2D(filters[1], filters[2], self.is_batchnorm)\n",
        "        self.temb3 = UNetTimeEmbedding(time_emb_dim, filters[2])\n",
        "        self.cemb3 = UNetTimeEmbedding(class_emb_dim, filters[2])\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv4 = UNetConv2D(filters[2], filters[3], self.is_batchnorm)\n",
        "        self.temb4 = UNetTimeEmbedding(time_emb_dim, filters[3])\n",
        "        self.cemb4 = UNetTimeEmbedding(class_emb_dim, filters[3])\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.center = UNetConv2D(filters[3], filters[4], self.is_batchnorm)\n",
        "        self.temb_center = UNetTimeEmbedding(time_emb_dim, filters[4])\n",
        "        self.cemb_center = UNetTimeEmbedding(class_emb_dim, filters[4])\n",
        "\n",
        "        # upsampling\n",
        "        self.up_concat4 = UNetUp(filters[4], filters[3], self.is_deconv, self.is_batchnorm)\n",
        "        self.up_temb4 = UNetTimeEmbedding(time_emb_dim, filters[3])\n",
        "        self.up_cemb4 = UNetTimeEmbedding(class_emb_dim, filters[3])\n",
        "\n",
        "        self.up_concat3 = UNetUp(filters[3], filters[2], self.is_deconv, self.is_batchnorm)\n",
        "        self.up_temb3 = UNetTimeEmbedding(time_emb_dim, filters[2])\n",
        "        self.up_cemb3 = UNetTimeEmbedding(class_emb_dim, filters[2])\n",
        "\n",
        "        self.up_concat2 = UNetUp(filters[2], filters[1], self.is_deconv, self.is_batchnorm)\n",
        "        self.up_temb2 = UNetTimeEmbedding(time_emb_dim, filters[1])\n",
        "        self.up_cemb2 = UNetTimeEmbedding(class_emb_dim, filters[1])\n",
        "\n",
        "        self.up_concat1 = UNetUp(filters[1], filters[0], self.is_deconv, self.is_batchnorm)\n",
        "        self.up_temb1 = UNetTimeEmbedding(time_emb_dim, filters[0])\n",
        "        self.up_cemb1 = UNetTimeEmbedding(class_emb_dim, filters[0])\n",
        "\n",
        "        # output\n",
        "        self.outconv1 = nn.Conv2d(filters[0], self.out_channels, 3, padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs, t, c=None):\n",
        "        t = self.time_embed(t)\n",
        "        if c is not None:\n",
        "            c = self.class_embed(c)\n",
        "        # inputs : [B, 1, 32, 32]\n",
        "\n",
        "        conv1 = self.conv1(inputs)  # [B, 64, 32, 32]\n",
        "        emb1 = self.temb1(t) + (self.cemb1(c) if c is not None else 0)\n",
        "        maxpool1 = self.maxpool1(conv1 + emb1)  # [B, 64, 16, 16]\n",
        "\n",
        "        conv2 = self.conv2(maxpool1)  # [B, 128, 16, 16]\n",
        "        emb2 = self.temb2(t) + (self.cemb2(c) if c is not None else 0)\n",
        "        maxpool2 = self.maxpool2(conv2 + emb2)  # [B, 128, 8, 8]\n",
        "\n",
        "        conv3 = self.conv3(maxpool2)  # [B, 256, 8, 8]\n",
        "        emb3 = self.temb3(t) + (self.cemb3(c) if c is not None else 0)\n",
        "        maxpool3 = self.maxpool3(conv3 + emb3)  # [B, 256, 4, 4]\n",
        "\n",
        "        conv4 = self.conv4(maxpool3)  # [B, 512, 4, 4]\n",
        "        emb4 = self.temb4(t) + (self.cemb4(c) if c is not None else 0)\n",
        "        maxpool4 = self.maxpool4(conv4 + emb4)  # [B, 512, 2, 2]\n",
        "\n",
        "        emb_center = self.temb_center(t) + (self.cemb_center(c) if c is not None else 0)\n",
        "        center = self.center(maxpool4) + emb_center # [B, 1024, 2, 2]\n",
        "\n",
        "\n",
        "        up_emb4 = self.up_temb4(t) + (self.up_cemb4(c) if c is not None else 0)\n",
        "        up4 = self.up_concat4(center, conv4) + up_emb4  # [B, 512, 4, 4]\n",
        "\n",
        "        up_emb3 = self.up_temb3(t) + (self.up_cemb3(c) if c is not None else 0)\n",
        "        up3 = self.up_concat3(up4, conv3) + up_emb3 # [B, 256, 8, 8]\n",
        "\n",
        "        up_emb2 = self.up_temb2(t) + (self.up_cemb2(c) if c is not None else 0)\n",
        "        up2 = self.up_concat2(up3, conv2) + up_emb2 # [B, 128, 16, 16]\n",
        "\n",
        "        up_emb1 = self.up_temb1(t) + (self.up_cemb1(c) if c is not None else 0)\n",
        "        up1 = self.up_concat1(up2, conv1) + up_emb1 # [B, 64, 32, 32]\n",
        "\n",
        "        out = self.outconv1(up1)  # [B, 1, 32, 32]\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdKlF90S9eIC"
      },
      "source": [
        "# DDPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn5dfm7k9eiG"
      },
      "outputs": [],
      "source": [
        "class DDPM:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps,\n",
        "        train_set=None,\n",
        "        test_set=None,\n",
        "        in_channels=1,\n",
        "        out_channels=1,\n",
        "        channel_scale=64,\n",
        "        train_batch_size=8,\n",
        "        test_batch_size=8\n",
        "    ):\n",
        "\n",
        "        self.n_timesteps = n_timesteps\n",
        "        self.channel_scale = channel_scale\n",
        "\n",
        "        # UNet for predicting total noise\n",
        "        self.g = UNet(in_channels=in_channels,\n",
        "                      out_channels=out_channels,\n",
        "                      n_steps=n_timesteps,\n",
        "                      channel_scale=channel_scale)\n",
        "        self.g = self.g.to(device)\n",
        "\n",
        "        # alpha, betas\n",
        "        self.noise_schedule = NoiseSchedule(n_timesteps=n_timesteps)\n",
        "\n",
        "        # forward encoder\n",
        "        self.encoder = ForwardEncoder(noise_schedule=self.noise_schedule)\n",
        "        self.decoder = ReverseDecoder(noise_schedule=self.noise_schedule, g=self.g)\n",
        "\n",
        "        # optimizer\n",
        "        self.lossFunction = torch.nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.g.parameters(), lr=0.0001)\n",
        "\n",
        "        # datasets\n",
        "        if train_set:\n",
        "            self.training_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
        "        if test_set:\n",
        "            self.testing_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    def save(self, path='./model.pt'):\n",
        "        torch.save(self.g.state_dict(), path)\n",
        "\n",
        "\n",
        "    def load(self, path='./model.pt'):\n",
        "        self.g.load_state_dict(torch.load(path))\n",
        "        self.g.eval()\n",
        "\n",
        "\n",
        "    def train_one_epoch(\n",
        "        self,\n",
        "        n_iter_limit = None,\n",
        "        p_uncond = 0.1\n",
        "    ):\n",
        "\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, data in enumerate(tqdm(self.training_loader)):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs, label = data\n",
        "            inputs = inputs.to(device)\n",
        "            # print(inputs.shape)\n",
        "\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            # sampled timestep and conditional variables\n",
        "            t = torch.randint(0, self.n_timesteps, (batch_size, )).to(device)\n",
        "            c = label.to(device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image, epsilon = self.encoder.noise(inputs, t)\n",
        "\n",
        "            outputs = None\n",
        "            if torch.rand((1, )).item() < p_uncond:\n",
        "                outputs = self.g(noised_image, t)\n",
        "            else:\n",
        "                outputs = self.g(noised_image, t, c)\n",
        "\n",
        "            loss = self.lossFunction(outputs, epsilon)\n",
        "\n",
        "            # Adjust learning weights\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i == n_iter_limit:\n",
        "                break\n",
        "\n",
        "        return running_loss / len(self.training_loader)\n",
        "\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        n_epoch=5,\n",
        "        n_iter_limit=None,\n",
        "        p_uncond=0.1\n",
        "    ):\n",
        "\n",
        "        history = []\n",
        "\n",
        "        for epoch in range(n_epoch):\n",
        "            print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "            # Make sure gradient tracking is on, and do a pass over the data\n",
        "            self.g.train(True)\n",
        "            avg_loss = self.train_one_epoch(n_iter_limit=n_iter_limit,\n",
        "                                            p_uncond=p_uncond)\n",
        "            history.append(avg_loss)\n",
        "            print('# epoch {} avg_loss: {}'.format(epoch + 1, avg_loss))\n",
        "\n",
        "            model_path = 'U{}_T{}_E{}.pt'.format(self.channel_scale,\n",
        "                                                             self.n_timesteps,\n",
        "                                                             epoch + 1)\n",
        "            torch.save(self.g.state_dict(), model_path)\n",
        "            torch.save(torch.tensor(history), 'history.pt')\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        epochs = None,\n",
        "        sampling_type = 'DDPM',\n",
        "        sampling_time_step = 10,\n",
        "        w = 0\n",
        "    ):\n",
        "        self.decoder.g = self.g\n",
        "        result = []\n",
        "        for i, data in enumerate(tqdm(self.testing_loader)):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs, label = data # data['image']\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            # timestep\n",
        "            t = torch.full((batch_size, ), self.n_timesteps - 1).to(device)\n",
        "            c = label.to(device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image, epsilon = self.encoder.noise(inputs, t)\n",
        "\n",
        "            # denoised image\n",
        "            denoised_image = None\n",
        "            if sampling_type == 'DDPM':\n",
        "                denoised_image = self.decoder.denoise(noised_image,\n",
        "                                                      self.n_timesteps,\n",
        "                                                      c=c,\n",
        "                                                      w=w)\n",
        "            if sampling_type == 'DDIM':\n",
        "                denoised_image = self.decoder.implicit_denoise(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w,\n",
        "                    sampling_time_step=sampling_time_step\n",
        "                )\n",
        "\n",
        "            result.append((inputs, noised_image, denoised_image))\n",
        "\n",
        "            if i == epochs - 1:\n",
        "                break\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_-bfyaE9-Zi"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyrjdgT3d_Yn"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 1000\n",
        "BATCH_SIZE = 384\n",
        "\n",
        "noise_schedule = NoiseSchedule(n_timesteps=TIME_STEPS, init_type='exponential')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBnmbCR7s054",
        "outputId": "65ba23bf-37ad-4fdf-f99b-0d5cc1080144"
      },
      "outputs": [],
      "source": [
        "# dataset = load_dataset(\"junyeong-nero/mnist_32by32\").with_format(\"torch\")\n",
        "# train, test = dataset['train'], dataset['test']\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test = MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2iEoXPDpFXh"
      },
      "outputs": [],
      "source": [
        "model = DDPM(\n",
        "    n_timesteps=TIME_STEPS,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    channel_scale=256,\n",
        "    train_set=train,\n",
        "    test_set=test,\n",
        "    train_batch_size=BATCH_SIZE,\n",
        "    test_batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4lkZWaVo24g",
        "outputId": "88db5747-4fbd-4197-d8e6-e38ace53e85e"
      },
      "outputs": [],
      "source": [
        "print(\"model size : \", sum(p.numel() for p in model.g.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "collapsed": true,
        "id": "Or-6gpJssqYx",
        "outputId": "0508894f-f3a0-44be-a47b-1b70c8c74eb2"
      },
      "outputs": [],
      "source": [
        "history = model.train(\n",
        "    n_epoch=30,\n",
        "    p_uncond=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qLvonFTss1g2",
        "outputId": "e4e56a1f-2304-474c-9104-7179b71cc17e"
      },
      "outputs": [],
      "source": [
        "print_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aH7UDHmqHEc"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = '/content/drive/My Drive/models/DDPM_MNIST/U256_T1000_E29_P02.pt'\n",
        "# MODEL_PATH = './U256_T1000_E30.pt'\n",
        "model.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqmVHtY_Zb3P"
      },
      "source": [
        "# Sampling: DDPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYoT-CNTynMx",
        "outputId": "b8251599-5743-41e9-b47d-5b1de1dbb920"
      },
      "outputs": [],
      "source": [
        "result_DDPM = model.evaluate(\n",
        "    epochs=5,\n",
        "    w=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HFNA7yOj0Tzw",
        "outputId": "da0dfe07-51d9-47d2-b774-d652a93ee325"
      },
      "outputs": [],
      "source": [
        "print_result(result_DDPM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP13xCsvZb3P"
      },
      "source": [
        "# Sampling: DDIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D6FCyFcVqHu",
        "outputId": "8fb09142-42cb-4239-fed0-891d61a898e9"
      },
      "outputs": [],
      "source": [
        "result_DDIM = model.evaluate(\n",
        "    epochs=5,\n",
        "    sampling_type='DDIM',\n",
        "    sampling_time_step=100,\n",
        "    w=0\n",
        ")\n",
        "# [num, 3, B, 1, 32, 32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aGPwTZ2urEYv",
        "outputId": "e4a15997-5d16-4571-eff9-e0135df65294"
      },
      "outputs": [],
      "source": [
        "print_result(result_DDIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs-ZM6l9dTfa"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm6VgklWaFG5"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-classification\",\n",
        "                model=\"farleyknight-org-username/vit-base-mnist\",\n",
        "                device=device)\n",
        "\n",
        "def inception_ViT(inputs):\n",
        "\n",
        "    def convert_to_pil(x):\n",
        "        res = []\n",
        "        for i in range(x.shape[0]):\n",
        "            res.append(to_pil_image(x[i]))\n",
        "        return res\n",
        "\n",
        "    def help(result):\n",
        "        res = [0.00000001] * 10\n",
        "        for r in result:\n",
        "            res[int(r['label'])] = r['score']\n",
        "        return res\n",
        "\n",
        "    # inputs : [B, 1, 32, 32]\n",
        "    out = pipe(convert_to_pil(inputs))\n",
        "    out = [help(x) for x in out]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMRHwhH55S-M"
      },
      "outputs": [],
      "source": [
        "# Function to get inception features\n",
        "def get_inception_features(inception_model, result):\n",
        "    target, origin = [], []\n",
        "\n",
        "    for image, noised_image, denoised_image in result:\n",
        "        # denoised_image : [B, 1, 32, 32]\n",
        "        origin += inception_model(image)\n",
        "        target += inception_model(denoised_image)\n",
        "\n",
        "    return origin, target\n",
        "\n",
        "# Calculate FID\n",
        "def calculate_fid(origin, target):\n",
        "    mu1, sigma1 = np.mean(origin, axis=0), np.cov(origin, rowvar=False)\n",
        "    mu2, sigma2 = np.mean(target, axis=0), np.cov(target, rowvar=False)\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "def calculate_inception_score(results):\n",
        "    scores = []\n",
        "    for part in results:\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores.append(np.exp(np.mean([np.sum(p * np.log(p / py)) for p in part])))\n",
        "    return np.mean(scores), np.std(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kCVY0MrejMw",
        "outputId": "6fe33880-a7ec-4c86-e446-5283ee29b987"
      },
      "outputs": [],
      "source": [
        "origin_DDPM, target_DDPM = get_inception_features(inception_ViT, result_DDPM)\n",
        "\n",
        "inception_score_mean, inception_score_std = calculate_inception_score(origin_DDPM)\n",
        "print(f'[Origin] IS: {inception_score_mean} ± {inception_score_std}')\n",
        "\n",
        "inception_score_mean, inception_score_std = calculate_inception_score(target_DDPM)\n",
        "print(f'[Target] IS: {inception_score_mean} ± {inception_score_std}')\n",
        "\n",
        "fid = calculate_fid(origin_DDPM, target_DDPM)\n",
        "print(f'FID: {fid}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz3qBMsM7aNf",
        "outputId": "dee6d08d-077f-445b-fbb9-6121678758e9"
      },
      "outputs": [],
      "source": [
        "origin_DDIM, target_DDIM = get_inception_features(inception_ViT, result_DDIM)\n",
        "\n",
        "inception_score_mean, inception_score_std = calculate_inception_score(origin_DDIM)\n",
        "print(f'[Origin] IS: {inception_score_mean} ± {inception_score_std}')\n",
        "\n",
        "inception_score_mean, inception_score_std = calculate_inception_score(target_DDIM)\n",
        "print(f'[Target] IS: {inception_score_mean} ± {inception_score_std}')\n",
        "\n",
        "fid = calculate_fid(origin_DDIM, target_DDIM)\n",
        "print(f'FID: {fid}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q_2fa4NhLEi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PqmVHtY_Zb3P"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
