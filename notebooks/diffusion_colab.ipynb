{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12671,"status":"ok","timestamp":1723437219258,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"SBCygEM5hLEa","outputId":"80416067-8db9-4640-f389-a960ad8fb16f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.2)\n"]}],"source":["!pip install --upgrade pip\n","!pip install -U -q torch torchvision scipy tdqm matplotlib scipy transformers"]},{"cell_type":"markdown","metadata":{"id":"aeVPd_CFqKVF"},"source":["# Google Drive Mount"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3134,"status":"ok","timestamp":1723437222389,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"ayIRMJrwqNR2","outputId":"06e32f7e-c1dd-436c-ac0f-ee8f416dd5bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"004v5ZfzxX6u"},"source":["# Import Modules"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4324,"status":"ok","timestamp":1723437226710,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"ikG5UIXCnvh6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import numpy as np\n","\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","import torchvision.transforms as transforms\n","from torchvision.transforms.functional import to_pil_image\n","\n","from scipy.linalg import sqrtm\n","import numpy as np\n","\n","from tqdm import tqdm\n","from matplotlib import pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"hptABxxsmq7g"},"source":["### Print Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723437226711,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"HJh_Om8QzgEY"},"outputs":[],"source":["def image_normalize(image):\n","    image = image.cpu()\n","    n_channels = image.shape[0]\n","    for channel in range(n_channels):\n","        max_value = torch.max(image[channel])\n","        min_value = torch.min(image[channel])\n","        image[channel] = (image[channel] - min_value) / (max_value - min_value)\n","\n","    image = image.permute(1, 2, 0)\n","\n","    return image\n","\n","def print_image(image):\n","    image = image_normalize(image)\n","    plt.figure(figsize=(5,5))\n","    plt.imshow(image)\n","    plt.show()\n","\n","def print_2images(image1, image2):\n","    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","    axes[0].imshow(image_normalize(image1))\n","    axes[0].set_title('Image 1')\n","\n","    axes[1].imshow(image_normalize(image2))\n","    axes[1].set_title('Image 2')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def print_digits(result):\n","    fig, axes = plt.subplots(1, 10, figsize=(10, 5))\n","\n","    B = result.shape[0]\n","    for i in range(B):\n","        axes[i].imshow(image_normalize(result[i]))\n","        axes[i].set_title(i)\n","        axes[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def print_result(result):\n","    for original_image, noised_image, denoised_image in result:\n","        batch_size = original_image.shape[0]\n","        for idx in range(batch_size):\n","            print_2images(original_image[idx], denoised_image[idx])\n","            # print_image(image[idx])\n","            # print_image(noised_image[idx])\n","            # print_image(denoised_image[idx])\n","\n","\n","def print_loss(loss_values):\n","    epochs = list(range(1, len(loss_values) + 1))\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(epochs, loss_values, 'b-o', label='Training Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Epoch vs Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dn0hElUKmq7g"},"source":["### Torch Device"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723437226711,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"KV83NB-7-Ueo"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"m7z-5moymq7h"},"source":["### Evaluations"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15977,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"FLjlFrOZmq7h","outputId":"2150b2b9-ced7-4534-9189-1a3014b0fab6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["from transformers import pipeline\n","\n","CLASSIFIER_MODEL = pipeline(\n","    \"image-classification\",\n","    model=\"farleyknight-org-username/vit-base-mnist\",\n","    device=device\n",")\n","\n","def inception_ViT(inputs):\n","\n","    def convert_to_pil(x):\n","        converted_images = []\n","        for i in range(x.shape[0]):\n","            converted_images.append(to_pil_image(x[i]))\n","        return converted_images\n","\n","    def convert_classifier_results(results):\n","        prob = [0.00000001] * 10\n","        for result in results:\n","            prob[int(result['label'])] = result['score']\n","        return prob\n","\n","    # inputs : [B, 1, 32, 32]\n","    out = CLASSIFIER_MODEL(convert_to_pil(inputs))\n","    out = [convert_classifier_results(x) for x in out]\n","    return out\n","\n","# Function to get inception features\n","def get_inception_features(inception_model, result):\n","    target, origin = [], []\n","\n","    for original_image, noised_image, denoised_image in result:\n","        # denoised_image : [B, 1, 32, 32]\n","        origin += inception_model(original_image)\n","        target += inception_model(denoised_image)\n","\n","    return origin, target\n","\n","# Calculate FID\n","def calculate_fid(origin, target):\n","    mu1, sigma1 = np.mean(origin, axis=0), np.cov(origin, rowvar=False)\n","    mu2, sigma2 = np.mean(target, axis=0), np.cov(target, rowvar=False)\n","    diff = mu1 - mu2\n","    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n","    if np.iscomplexobj(covmean):\n","        covmean = covmean.real\n","    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n","    return fid\n","\n","def calculate_inception_score(results):\n","    scores = []\n","    for part in results:\n","        py = np.mean(part, axis=0)\n","        scores.append(np.exp(np.mean([np.sum(p * np.log(p / py)) for p in part])))\n","    return np.mean(scores), np.std(scores)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"lWXFvNsVmq7h"},"outputs":[],"source":["def print_scores(result):\n","    origin, target = get_inception_features(inception_ViT, result)\n","\n","    origin_IS_mean, origin_IS_std = calculate_inception_score(origin)\n","    print(f'[Origin] IS: {origin_IS_mean} ± {origin_IS_std}')\n","\n","    target_IS_mean, target_IS_std = calculate_inception_score(target)\n","    print(f'[Target] IS: {target_IS_mean} ± {target_IS_std}')\n","\n","    FID = calculate_fid(origin, target)\n","    print(f'FID: {FID}')\n","\n","    return origin_IS_mean, target_IS_mean, FID"]},{"cell_type":"markdown","metadata":{"id":"BHS6FSDSn2hy"},"source":["# Noise Scheduler\n","- betas, alphas"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"oq0_qYUenyDg"},"outputs":[],"source":["class NoiseSchedule:\n","\n","    def __init__(\n","        self,\n","        n_timesteps,\n","        beta_start = 0.0001,\n","        beta_end = 0.02,\n","        device = device,\n","        init_type = \"linear\"\n","    ) -\u003e None:\n","\n","        self._size = n_timesteps\n","        if init_type == \"linear\":\n","            self._betas = torch.linspace(beta_start, beta_end, n_timesteps).to(device)\n","        if init_type == \"exponential\":\n","            self._betas = torch.from_numpy(np.geomspace(beta_start, beta_end, n_timesteps)).to(device)\n","        self._alphas = self._calculate_alphas()\n","\n","    def _calculate_alphas(self):\n","        self._alphas = torch.cumprod(1 - self._betas, axis=0)\n","        return self._alphas"]},{"cell_type":"markdown","metadata":{"id":"P8PxHOV6n-ZV"},"source":["# Forward Encoder"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"eHr0pKLWn5tI"},"outputs":[],"source":["class ForwardEncoder:\n","\n","    def __init__(self, noise_schedule) -\u003e None:\n","        self.noise_schedule = noise_schedule\n","\n","    def noise(self, data, time_step):\n","        # time_step : [B]\n","        # data : [B, 1, 32, 32]\n","\n","        alpha = self.noise_schedule._alphas[time_step]\n","        alpha = alpha.reshape(-1, 1, 1, 1)\n","        # alpha : [B, 1, 1, 1]\n","\n","        epsilon = torch.randn(data.shape).to(device)\n","        # torch.randn ~ N(0, 1)\n","\n","        return torch.sqrt(alpha) * data + torch.sqrt(1 - alpha) * epsilon, epsilon"]},{"cell_type":"markdown","metadata":{"id":"1BIC7rsQ8en-"},"source":["# Reverse Decoder"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"gV7lE5EZzyuy"},"outputs":[],"source":["import torch\n","\n","class ReverseDecoder:\n","\n","    def __init__(\n","        self,\n","        noise_schedule,\n","        g\n","    ) -\u003e None:\n","        self.noise_schedule = noise_schedule\n","        self.g = g\n","\n","    def DDPM_sampling(\n","        self,\n","        noise_data,\n","        time_step,\n","        c = None,\n","        w = 0\n","    ):\n","        # noise_data : [B, 1, 32, 32]\n","        # c : [B]\n","        # time_step : INT\n","\n","        batch_size = noise_data.shape[0]\n","        # batch_size : B\n","\n","        with torch.no_grad():\n","\n","            # step : [T - 1, T - 2, .. 2, 1, 0]\n","            for step in range(time_step - 1, -1, -1):\n","\n","                t = torch.full((batch_size, ), step).to(device)\n","                t = t.reshape(-1, 1, 1, 1)\n","                # t : [B, 1, 1, 1]\n","\n","                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n","                mu = 1 / torch.sqrt(1 - self.noise_schedule._betas[t]) * (noise_data - (self.noise_schedule._betas[t] / (1 - self.noise_schedule._alphas[t])) * predict_noise)\n","                # mu : [B, 1, 32, 32]\n","\n","                if step == 0:\n","                    # if t == 0, no add noise\n","                    break\n","\n","                epsilon = torch.randn(noise_data.shape).to(device)\n","                # epsilon : [B, 1, 32, 32]\n","\n","                noise_data = mu + torch.sqrt(self.noise_schedule._betas[t]) * epsilon\n","                # noise_data : [B, 1, 32, 32]\n","\n","        return noise_data\n","\n","    def DDIM_sampling(\n","        self,\n","        noise_data,\n","        time_step,\n","        c = None,\n","        w = 0,\n","        sampling_time_step = 10,\n","        custom_sampling_steps = None\n","    ):\n","        # noise_data : [B, 1, 32, 32]\n","        # c : [B]\n","        # time_step : INT\n","\n","        batch_size = noise_data.shape[0]\n","        tau = list(range(0, time_step, time_step // sampling_time_step))\n","        if custom_sampling_steps is not None:\n","            tau = custom_sampling_steps\n","\n","        S = len(tau)\n","\n","        # batch_size : B\n","        with torch.no_grad():\n","\n","            # step : [T - 1, T - 2, .. 2, 1, 0]\n","            for i in range(S - 1, -1, -1):\n","\n","                t = torch.full((batch_size, ), tau[i]).to(device)\n","                t = t.reshape(-1, 1, 1, 1)\n","                alpha_t = self.noise_schedule._alphas[t]\n","\n","                alpha_t_1 = torch.full((batch_size, 1, 1, 1,), 1).to(device)\n","                if i - 1 \u003e= 0:\n","                    t_1 = torch.full((batch_size, ), tau[i - 1]).to(device)\n","                    t_1 = t_1.reshape(-1, 1, 1, 1)\n","                    alpha_t_1 = self.noise_schedule._alphas[t_1]\n","\n","                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n","                first = torch.sqrt(alpha_t_1) * ((noise_data - torch.sqrt(1 - alpha_t) * predict_noise) / torch.sqrt(alpha_t))\n","                second = torch.sqrt(1 - alpha_t_1) * predict_noise\n","\n","                noise_data = first + second\n","\n","        return noise_data"]},{"cell_type":"markdown","metadata":{"id":"c0pbKzSh82Og"},"source":["# UNet"]},{"cell_type":"markdown","metadata":{"id":"WG_ujnAqOf-H"},"source":["### Backbones"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"NSR1El6-Of-I"},"outputs":[],"source":["class PositionalEmbedding(nn.Module):\n","\n","    def __init__(self, num_steps, time_emb_dim) -\u003e None:\n","        super(PositionalEmbedding, self).__init__()\n","\n","        self.time_embed = nn.Embedding(num_steps, time_emb_dim)\n","        self.time_embed.weight.data = self.sinusoidal_embedding(num_steps, time_emb_dim)\n","        self.time_embed.requires_grad_(False)\n","\n","    def sinusoidal_embedding(self, n, d):\n","        # Returns the standard positional embedding\n","        embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n","        sin_mask = torch.arange(0, n, 2)\n","        embedding[sin_mask] = torch.sin(embedding[sin_mask])\n","        embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n","\n","        return embedding\n","\n","    def forward(self, input):\n","        return self.time_embed(input)\n","\n","\n","class WideResNetBlock(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        is_batchnorm = True,\n","        n = 3,\n","        kernel_size = 3,\n","        stride = 1,\n","        padding = 1,\n","        num_groups = 32\n","    ):\n","        super(WideResNetBlock, self).__init__()\n","        self.n = n\n","        self.ks = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","\n","        self.shortcut = nn.Sequential()\n","        if kernel_size != 1 or in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n","                # nn.BatchNorm2d(out_size)\n","            )\n","\n","        if is_batchnorm:\n","            for i in range(1, n + 1):\n","                conv = nn.Sequential(\n","                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n","                    #  nn.BatchNorm2d(out_size),\n","                    nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n","                    nn.SiLU(inplace=True)\n","                )\n","                setattr(self, 'conv%d' % i, conv)\n","                in_channels = out_channels\n","\n","        else:\n","            for i in range(1, n + 1):\n","                conv = nn.Sequential(\n","                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n","                    nn.SiLU(inplace=True)\n","                )\n","                setattr(self, 'conv%d' % i, conv)\n","                in_channels = out_channels\n","\n","\n","    def forward(self, inputs):\n","        x = inputs\n","        for i in range(1, self.n + 1):\n","            conv = getattr(self, 'conv%d' % i)\n","            x = conv(x)\n","        x += self.shortcut(inputs)\n","        return x\n","\n","\n","class MultiHeadAttentionBlock(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        is_batchnorm = True,\n","        num_heads = 2,\n","        num_groups = 32,\n","    ):\n","        super(MultiHeadAttentionBlock, self).__init__()\n","\n","        self.is_batchnorm = is_batchnorm\n","        # For each of heads use d_k = d_v = d_model / num_heads\n","        self.num_heads = num_heads\n","        self.d_model = out_channels\n","        self.d_keys = out_channels // num_heads\n","        self.d_values = out_channels // num_heads\n","\n","        self.W_Q = nn.Linear(in_channels, out_channels, bias=False)\n","        self.W_K = nn.Linear(in_channels, out_channels, bias=False)\n","        self.W_V = nn.Linear(in_channels, out_channels, bias=False)\n","\n","        self.final_projection = nn.Linear(out_channels, out_channels, bias=False)\n","        self.norm = nn.GroupNorm(num_channels=out_channels, num_groups=num_groups)\n","\n","    def split_features_for_heads(self, tensor):\n","        batch, hw, emb_dim = tensor.shape\n","        channels_per_head = emb_dim // self.num_heads\n","        heads_splitted_tensor = torch.split(tensor, split_size_or_sections=channels_per_head, dim=-1)\n","        heads_splitted_tensor = torch.stack(heads_splitted_tensor, 1)\n","        return heads_splitted_tensor\n","\n","    def attention(self, q, k, v):\n","\n","        B, C, H, W = q.shape\n","        q = q.view(B, C, q.shape[2] * q.shape[3]).transpose(1, 2)\n","        k = k.view(B, C, k.shape[2] * k.shape[3]).transpose(1, 2)\n","        v = v.view(B, C, v.shape[2] * v.shape[3]).transpose(1, 2)\n","\n","        # [B, H * W, C_in]\n","\n","        q = self.W_Q(q)\n","        k = self.W_K(k)\n","        v = self.W_V(v)\n","        # N = H * W\n","        # [B, N, C_out]\n","\n","        Q = self.split_features_for_heads(q)\n","        K = self.split_features_for_heads(k)\n","        V = self.split_features_for_heads(v)\n","        # [B, num_heads, N, C_out / num_heads]\n","\n","        scale = self.d_keys ** -0.5\n","        attention_scores = torch.softmax(torch.matmul(Q, K.transpose(-1, -2)) * scale, dim=-1)\n","        attention_scores = torch.matmul(attention_scores, V)\n","        # [B, num_heads, N, C_out / num_heads]\n","\n","        attention_scores = attention_scores.permute(0, 2, 1, 3).contiguous()\n","        # [B, num_heads, N, C_out / num_heads] --\u003e [B, N, num_heads, C_out / num_heads]\n","\n","        concatenated_heads_attention_scores = attention_scores.view(B, H * W, self.d_model)\n","        # [B, N, num_heads, C_out / num_heads] --\u003e [batch, N, C_out]\n","\n","        linear_projection = self.final_projection(concatenated_heads_attention_scores)\n","        linear_projection = linear_projection.transpose(-1, -2).reshape(B, self.d_model, H, W)\n","        # [B, N, C_out] -\u003e [B, C_out, N] -\u003e [B, C_out, H, W]\n","\n","        # Residual connection + norm\n","        out = linear_projection\n","        if self.is_batchnorm:\n","            v = v.transpose(-1, -2).reshape(B, self.d_model, H, W)\n","            out = self.norm(out + v)\n","        return out\n","\n","    def forward(self, q, k, v):\n","        return self.attention(q, k, v)\n","\n","\n","class SelfAttentionBlock(MultiHeadAttentionBlock):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        is_batchnorm = True,\n","        num_heads = 2,\n","        num_groups = 32,\n","    ):\n","        super().__init__(in_channels, out_channels, num_heads, num_groups)\n","\n","\n","    def forward(self, x):\n","        return super().forward(x, x, x)"]},{"cell_type":"markdown","metadata":{"id":"1eJz64ADOf-I"},"source":["### UNet Body"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"olBnE6_v81Nf"},"outputs":[],"source":["class UNetDown(nn.Module):\n","\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        base_model = WideResNetBlock,\n","        is_deconv = True,\n","        is_batchnorm = True\n","    ):\n","        super(UNetDown, self).__init__()\n","        self.conv = base_model(in_channels, out_channels, is_batchnorm=is_batchnorm)\n","\n","    def forward(self, input):\n","        return self.conv(input)\n","\n","\n","class UNetUp(nn.Module):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        base_model = WideResNetBlock,\n","        is_deconv = True,\n","        is_batchnorm = True\n","    ):\n","        super(UNetUp, self).__init__()\n","        self.conv = base_model(out_channels * 2, out_channels, is_batchnorm=is_batchnorm)\n","        if is_deconv:\n","            self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n","        else:\n","            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n","\n","    def forward(self, inputs0, *input):\n","        outputs0 = self.up(inputs0)\n","        for i in range(len(input)):\n","            outputs0 = torch.cat([outputs0, input[i]], 1)\n","        return self.conv(outputs0)\n","\n","\n","class UNetTimeEmbedding(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out) -\u003e None:\n","        super(UNetTimeEmbedding, self).__init__()\n","        self.ln = nn.Linear(dim_in, dim_out)\n","        self.activation = nn.SiLU()\n","        self.ln2 = nn.Linear(dim_out, dim_out)\n","\n","\n","    def forward(self, inputs):\n","        B = inputs.shape[0]\n","\n","        x = self.ln(inputs)\n","        x = self.activation(x)\n","        x = self.ln2(x)\n","\n","        return x.reshape(B, -1, 1, 1)\n","\n","\n","class UNet(nn.Module):\n","\n","    def __init__(\n","        self,\n","        in_channels = 1,\n","        out_channels = 1,\n","        n_steps = 1000,\n","        time_emb_dim = 256,\n","        n_classes = 10,\n","        class_emb_dim = 64,\n","        channel_scale = 64,\n","        num_channel_scale = 5,\n","        custom_channel_scale = None,\n","        cross_attention_layer_indices = [-1],\n","        self_attention_layer_indices = [-1],\n","        is_deconv = True,\n","        is_batchnorm = True\n","    ):\n","        super(UNet, self).__init__()\n","        self.is_deconv = is_deconv\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.is_batchnorm = is_batchnorm\n","\n","        self.self_attention_layer_indices = set(self_attention_layer_indices)\n","        self.cross_attention_layer_indices = set(cross_attention_layer_indices)\n","\n","        # time embedding\n","        self.time_embedding = PositionalEmbedding(n_steps, time_emb_dim)\n","\n","        # conditional variable embedding\n","        self.context_embedding = PositionalEmbedding(n_classes, class_emb_dim)\n","\n","        if custom_channel_scale is None:\n","            # channel exponenetial scales with `channel_scale`\n","            # 64, 128, 256, 512, 1024\n","            filters = [channel_scale * (2 ** i) for i in range(num_channel_scale)]\n","        else:\n","            # custom channel scales\n","            num_channel_scale = len(custom_channel_scale)\n","            filters = custom_channel_scale\n","\n","        self.num_layers = num_channel_scale\n","\n","        # Downsampling\n","        filters.insert(0, in_channels)\n","\n","        for layer_idx in range(1, self.num_layers):\n","            base_model = WideResNetBlock\n","            if (layer_idx - self.num_layers) in self.self_attention_layer_indices:\n","                base_model = SelfAttentionBlock\n","\n","            conv = UNetDown(in_channels=filters[layer_idx - 1],\n","                            out_channels=filters[layer_idx],\n","                            is_batchnorm=self.is_batchnorm,\n","                            base_model=base_model)\n","            temb = UNetTimeEmbedding(time_emb_dim, filters[layer_idx])\n","            cemb = UNetTimeEmbedding(class_emb_dim, filters[layer_idx])\n","            maxpool = nn.MaxPool2d(kernel_size=2)\n","            if (layer_idx - self.num_layers) in self.cross_attention_layer_indices:\n","                cross_attention = MultiHeadAttentionBlock(\n","                    in_channels=filters[layer_idx],\n","                    out_channels=filters[layer_idx],\n","                    is_batchnorm=False\n","                )\n","                setattr(self, 'down_cross_attention%d' % layer_idx, cross_attention)\n","\n","            setattr(self, 'down_conv%d' % layer_idx, conv)\n","            setattr(self, 'down_temb%d' % layer_idx, temb)\n","            setattr(self, 'down_cemb%d' % layer_idx, cemb)\n","            setattr(self, 'down_maxpool%d' % layer_idx, maxpool)\n","\n","\n","        # Bottleneck\n","\n","        self.center = UNetDown(filters[-2], filters[-1], is_batchnorm=self.is_batchnorm)\n","        self.temb_center = UNetTimeEmbedding(time_emb_dim, filters[-1])\n","        self.cemb_center = UNetTimeEmbedding(class_emb_dim, filters[-1])\n","        self.cross_attention_center = MultiHeadAttentionBlock(\n","            in_channels=filters[-1],\n","            out_channels=filters[-1],\n","            is_batchnorm=False\n","        )\n","\n","        # upsampling\n","        filters[0] = out_channels\n","\n","        for layer_idx in range(1, self.num_layers):\n","            base_model = WideResNetBlock\n","            if (layer_idx - self.num_layers) in self.self_attention_layer_indices:\n","                base_model = SelfAttentionBlock\n","            conv = UNetUp(filters[layer_idx + 1],\n","                          filters[layer_idx],\n","                          is_deconv=self.is_deconv,\n","                          is_batchnorm=self.is_batchnorm,\n","                          base_model=base_model)\n","            temb = UNetTimeEmbedding(time_emb_dim, filters[layer_idx])\n","\n","            setattr(self, 'up_conv%d' % layer_idx, conv)\n","            setattr(self, 'up_temb%d' % layer_idx, temb)\n","\n","        # output\n","        self.outconv = nn.Conv2d(filters[1], self.out_channels, 3, padding=1)\n","\n","\n","    def forward(\n","        self,\n","        inputs,\n","        t,\n","        c = None\n","    ):\n","\n","        t = self.time_embedding(t)\n","        if c is not None:\n","            c = self.context_embedding(c)\n","\n","        # inputs : [B, 1, 32, 32]\n","\n","        x = inputs\n","        downsampling_result = [None]\n","\n","        # DOWN-SAMPLING\n","        for layer_idx in range(1, self.num_layers):\n","\n","            conv = getattr(self, 'down_conv%d' % layer_idx)\n","            temb = getattr(self, 'down_temb%d' % layer_idx)\n","            cemb = getattr(self, 'down_cemb%d' % layer_idx)\n","            maxpool = getattr(self, 'down_maxpool%d' % layer_idx)\n","\n","            x = conv(x)\n","            downsampling_result.append(x)\n","\n","            if c is not None and (layer_idx - self.num_layers) in self.cross_attention_layer_indices:\n","                CA = getattr(self, 'down_cross_attention%d' % layer_idx)\n","                context_emb = cemb(c)\n","                x = CA(x, context_emb, context_emb)\n","\n","            x += temb(t)\n","            x = maxpool(x)\n","\n","        # BOTTLENECK\n","\n","        x = self.center(x)\n","        if c is not None:\n","            context_emb = self.cemb_center(c)\n","            x = self.cross_attention_center(x, context_emb, context_emb)\n","        x += self.temb_center(t)\n","\n","        # UP-SAMPLING\n","\n","        for layer_idx in range(self.num_layers - 1, 0, -1):\n","            conv = getattr(self, 'up_conv%d' % layer_idx)\n","            temb = getattr(self, 'up_temb%d' % layer_idx)\n","            x = conv(x, downsampling_result[layer_idx])\n","            x += temb(t)\n","\n","        return self.outconv(x)"]},{"cell_type":"markdown","metadata":{"id":"CdKlF90S9eIC"},"source":["# Diffusion"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"Zn5dfm7k9eiG"},"outputs":[],"source":["class MyDiffusion:\n","\n","    def __init__(\n","        self,\n","        n_timesteps,\n","        train_set = None,\n","        test_set = None,\n","        in_channels = 1,\n","        out_channels = 1,\n","        channel_scale = 64,\n","        num_channle_scale = 5,\n","        train_batch_size = 8,\n","        test_batch_size = 8,\n","        custom_channel_scale = None,\n","        learning_rate = 0.0001\n","    ):\n","\n","        self.n_timesteps = n_timesteps\n","        self.channel_scale = channel_scale\n","\n","        # UNet for predicting total noise\n","        self.g = UNet(in_channels=in_channels,\n","                      out_channels=out_channels,\n","                      n_steps=n_timesteps,\n","                      channel_scale=channel_scale,\n","                      num_channel_scale=num_channle_scale,\n","                      custom_channel_scale=custom_channel_scale)\n","        self.g = self.g.to(device)\n","\n","        # alpha, betas\n","        self.noise_schedule = NoiseSchedule(n_timesteps=n_timesteps)\n","\n","        # forward encoder\n","        self.encoder = ForwardEncoder(noise_schedule=self.noise_schedule)\n","        self.decoder = ReverseDecoder(noise_schedule=self.noise_schedule, g=self.g)\n","\n","        # optimizer\n","        self.lossFunction = torch.nn.MSELoss()\n","        self.optimizer = torch.optim.Adam(self.g.parameters(), lr=learning_rate)\n","\n","        # datasets\n","        if train_set:\n","            self.training_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n","        if test_set:\n","            self.testing_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n","\n","\n","    def save(self, path='./model.pt'):\n","        torch.save(self.g.state_dict(), path)\n","\n","\n","    def load(self, path='./model.pt'):\n","        self.g.load_state_dict(torch.load(path))\n","        self.g.eval()\n","\n","\n","    def train_one_epoch(\n","        self,\n","        n_iter_limit = None,\n","        p_uncond = 0.1\n","    ):\n","\n","        running_loss = 0\n","\n","        for i, data in enumerate(tqdm(self.training_loader)):\n","\n","            # inputs = [B, 1, 32, 32]\n","            inputs, label = data\n","            inputs = inputs.to(device)\n","            # print(inputs.shape)\n","\n","            batch_size = inputs.shape[0]\n","\n","            # sampled timestep and conditional variables\n","            t = torch.randint(0, self.n_timesteps, (batch_size, )).to(device)\n","            c = label.to(device)\n","\n","            # outputs = [B, 1, 28, 28]\n","            noised_image, epsilon = self.encoder.noise(inputs, t)\n","\n","            outputs = None\n","            if torch.rand((1, )).item() \u003c p_uncond:\n","                outputs = self.g(noised_image, t)\n","            else:\n","                outputs = self.g(noised_image, t, c)\n","\n","            loss = self.lossFunction(outputs, epsilon)\n","\n","            # Adjust learning weights\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            if i == n_iter_limit:\n","                break\n","\n","        return running_loss / len(self.training_loader)\n","\n","\n","    def train(\n","        self,\n","        n_epoch = 5,\n","        n_iter_limit = None,\n","        p_uncond = 0.1\n","    ):\n","\n","        history = []\n","\n","        for epoch in range(n_epoch):\n","            print('EPOCH {}:'.format(epoch + 1))\n","\n","            # Make sure gradient tracking is on, and do a pass over the data\n","            self.g.train(True)\n","            avg_loss = self.train_one_epoch(n_iter_limit=n_iter_limit,\n","                                            p_uncond=p_uncond)\n","            history.append(avg_loss)\n","            print('# epoch {} avg_loss: {}'.format(epoch + 1, avg_loss))\n","\n","            model_path = 'U{}_T{}_E{}.pt'.format(self.channel_scale,\n","                                                             self.n_timesteps,\n","                                                             epoch + 1)\n","            torch.save(self.g.state_dict(), model_path)\n","            torch.save(torch.tensor(history), 'history.pt')\n","\n","        return history\n","\n","\n","    def evaluate(\n","        self,\n","        epochs = None,\n","        sampling_type = \"DDPM\",\n","        sampling_time_step = 10,\n","        w = 0\n","    ):\n","        self.decoder.g = self.g\n","        result = []\n","        for i, data in enumerate(tqdm(self.testing_loader)):\n","\n","            # inputs = [B, 1, 32, 32]\n","            inputs, label = data # data['image']\n","            inputs = inputs.to(device)\n","\n","            batch_size = inputs.shape[0]\n","\n","            # timestep\n","            t = torch.full((batch_size, ), self.n_timesteps - 1).to(device)\n","            c = label.to(device)\n","\n","            # outputs = [B, 1, 28, 28]\n","            noised_image, epsilon = self.encoder.noise(inputs, t)\n","\n","            # denoised image\n","            denoised_image = None\n","            if sampling_type == \"DDPM\":\n","                denoised_image = self.decoder.DDPM_sampling(\n","                    noised_image,\n","                    self.n_timesteps,\n","                    c=c,\n","                    w=w\n","                )\n","            if sampling_type == \"DDIM\":\n","                denoised_image = self.decoder.DDIM_sampling(\n","                    noised_image,\n","                    self.n_timesteps,\n","                    c=c,\n","                    w=w,\n","                    sampling_time_step=sampling_time_step\n","                )\n","\n","            result.append((inputs, noised_image, denoised_image))\n","\n","            if i == epochs - 1:\n","                break\n","\n","        return result"]},{"cell_type":"markdown","metadata":{"id":"z_-bfyaE9-Zi"},"source":["# Train"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"gyrjdgT3d_Yn"},"outputs":[],"source":["TIME_STEPS = 1000\n","BATCH_SIZE = 512\n","EPOCHS = 30\n","P_UNCOND = 0.1\n","\n","noise_schedule = NoiseSchedule(\n","    n_timesteps=TIME_STEPS,\n","    init_type=\"exponential\"\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723437242685,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"jBnmbCR7s054"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize(32),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5], [0.5])\n","])\n","\n","train = MNIST(root='./data', train=True, download=True, transform=transform)\n","test = MNIST(root='./data', train=False, download=True, transform=transform)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1385,"status":"ok","timestamp":1723437244066,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"m2iEoXPDpFXh"},"outputs":[],"source":["model = MyDiffusion(\n","    n_timesteps=TIME_STEPS,\n","    in_channels=1,\n","    out_channels=1,\n","    custom_channel_scale=[128, 128, 256, 256, 512, 512],\n","    train_set=train,\n","    test_set=test,\n","    train_batch_size=BATCH_SIZE,\n","    test_batch_size=8\n",")\n","\n","# MODEL_PATH = '/content/drive/My Drive/models/DDPM_MNIST/UCA128_T1000_E30.pt'\n","# model.load(MODEL_PATH)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":11,"status":"ok","timestamp":1723437244066,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"tq8fF7Vfs9fI","outputId":"414e5d3d-8a48-4086-cd41-cb4977b2f3c4"},"outputs":[{"data":{"text/plain":["UNet(\n","  (time_embedding): PositionalEmbedding(\n","    (time_embed): Embedding(1000, 256)\n","  )\n","  (context_embedding): PositionalEmbedding(\n","    (time_embed): Embedding(10, 64)\n","  )\n","  (down_conv1): UNetDown(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(1, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","  )\n","  (down_temb1): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=128, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=128, out_features=128, bias=True)\n","  )\n","  (down_cemb1): UNetTimeEmbedding(\n","    (ln): Linear(in_features=64, out_features=128, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=128, out_features=128, bias=True)\n","  )\n","  (down_maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (down_conv2): UNetDown(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","  )\n","  (down_temb2): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=128, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=128, out_features=128, bias=True)\n","  )\n","  (down_cemb2): UNetTimeEmbedding(\n","    (ln): Linear(in_features=64, out_features=128, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=128, out_features=128, bias=True)\n","  )\n","  (down_maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (down_conv3): UNetDown(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","  )\n","  (down_temb3): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=256, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=256, out_features=256, bias=True)\n","  )\n","  (down_cemb3): UNetTimeEmbedding(\n","    (ln): Linear(in_features=64, out_features=256, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=256, out_features=256, bias=True)\n","  )\n","  (down_maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (down_conv4): UNetDown(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","  )\n","  (down_temb4): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=256, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=256, out_features=256, bias=True)\n","  )\n","  (down_cemb4): UNetTimeEmbedding(\n","    (ln): Linear(in_features=64, out_features=256, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=256, out_features=256, bias=True)\n","  )\n","  (down_maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (down_cross_attention5): MultiHeadAttentionBlock(\n","    (W_Q): Linear(in_features=512, out_features=512, bias=False)\n","    (W_K): Linear(in_features=512, out_features=512, bias=False)\n","    (W_V): Linear(in_features=512, out_features=512, bias=False)\n","    (final_projection): Linear(in_features=512, out_features=512, bias=False)\n","    (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n","  )\n","  (down_conv5): UNetDown(\n","    (conv): SelfAttentionBlock(\n","      (W_Q): Linear(in_features=256, out_features=512, bias=False)\n","      (W_K): Linear(in_features=256, out_features=512, bias=False)\n","      (W_V): Linear(in_features=256, out_features=512, bias=False)\n","      (final_projection): Linear(in_features=512, out_features=512, bias=False)\n","      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n","    )\n","  )\n","  (down_temb5): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=512, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (down_cemb5): UNetTimeEmbedding(\n","    (ln): Linear(in_features=64, out_features=512, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (down_maxpool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (center): UNetDown(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","  )\n","  (temb_center): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=512, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (cemb_center): UNetTimeEmbedding(\n","    (ln): Linear(in_features=64, out_features=512, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (cross_attention_center): MultiHeadAttentionBlock(\n","    (W_Q): Linear(in_features=512, out_features=512, bias=False)\n","    (W_K): Linear(in_features=512, out_features=512, bias=False)\n","    (W_V): Linear(in_features=512, out_features=512, bias=False)\n","    (final_projection): Linear(in_features=512, out_features=512, bias=False)\n","    (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n","  )\n","  (up_conv1): UNetUp(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","    (up): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  )\n","  (up_temb1): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=128, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=128, out_features=128, bias=True)\n","  )\n","  (up_conv2): UNetUp(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","    (up): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  )\n","  (up_temb2): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=128, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=128, out_features=128, bias=True)\n","  )\n","  (up_conv3): UNetUp(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","    (up): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  )\n","  (up_temb3): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=256, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=256, out_features=256, bias=True)\n","  )\n","  (up_conv4): UNetUp(\n","    (conv): WideResNetBlock(\n","      (shortcut): Sequential(\n","        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      )\n","      (conv1): Sequential(\n","        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv2): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","      (conv3): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): SiLU(inplace=True)\n","      )\n","    )\n","    (up): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  )\n","  (up_temb4): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=256, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=256, out_features=256, bias=True)\n","  )\n","  (up_conv5): UNetUp(\n","    (conv): SelfAttentionBlock(\n","      (W_Q): Linear(in_features=1024, out_features=512, bias=False)\n","      (W_K): Linear(in_features=1024, out_features=512, bias=False)\n","      (W_V): Linear(in_features=1024, out_features=512, bias=False)\n","      (final_projection): Linear(in_features=512, out_features=512, bias=False)\n","      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n","    )\n","    (up): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  )\n","  (up_temb5): UNetTimeEmbedding(\n","    (ln): Linear(in_features=256, out_features=512, bias=True)\n","    (activation): SiLU()\n","    (ln2): Linear(in_features=512, out_features=512, bias=True)\n","  )\n","  (outconv): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model.g"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1723437244066,"user":{"displayName":"송준영","userId":"17760458705363094278"},"user_tz":-540},"id":"Q4lkZWaVo24g","outputId":"0925d45a-3012-4efc-c363-3357c825491c"},"outputs":[{"name":"stdout","output_type":"stream","text":["model size :  33119361\n"]}],"source":["print(\"model size : \", sum(p.numel() for p in model.g.parameters() if p.requires_grad))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"vLwQ3_CdsgnQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH 1:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:57\u003c00:00,  1.50s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 1 avg_loss: 0.10173031315207481\n","EPOCH 2:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:56\u003c00:00,  1.50s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 2 avg_loss: 0.04090560980583147\n","EPOCH 3:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 3 avg_loss: 0.03239023780166093\n","EPOCH 4:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 4 avg_loss: 0.0286735827576811\n","EPOCH 5:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 5 avg_loss: 0.0262330088259305\n","EPOCH 6:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 6 avg_loss: 0.025262549456398366\n","EPOCH 7:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 7 avg_loss: 0.0236861732299045\n","EPOCH 8:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 8 avg_loss: 0.022577180211448063\n","EPOCH 9:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 9 avg_loss: 0.02179876476589401\n","EPOCH 10:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:58\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 10 avg_loss: 0.02100499057984453\n","EPOCH 11:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 11 avg_loss: 0.020685656264550604\n","EPOCH 12:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 12 avg_loss: 0.02006019702402212\n","EPOCH 13:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 13 avg_loss: 0.019498387304276735\n","EPOCH 14:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 14 avg_loss: 0.019454889394090336\n","EPOCH 15:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:58\u003c00:00,  1.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 15 avg_loss: 0.019538655072071795\n","EPOCH 16:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 16 avg_loss: 0.019031452472811027\n","EPOCH 17:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 17 avg_loss: 0.018619843853353444\n","EPOCH 18:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 18 avg_loss: 0.018476732540875673\n","EPOCH 19:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:58\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 19 avg_loss: 0.01801321968057398\n","EPOCH 20:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:58\u003c00:00,  1.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 20 avg_loss: 0.017901174850383048\n","EPOCH 21:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 21 avg_loss: 0.017972929324260204\n","EPOCH 22:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:58\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 22 avg_loss: 0.01787637940497469\n","EPOCH 23:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 23 avg_loss: 0.017609532946184024\n","EPOCH 24:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:58\u003c00:00,  1.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 24 avg_loss: 0.017578929315431643\n","EPOCH 25:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 25 avg_loss: 0.017587503299937916\n","EPOCH 26:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 26 avg_loss: 0.01720581699352143\n","EPOCH 27:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 27 avg_loss: 0.017224670664044255\n","EPOCH 28:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [03:00\u003c00:00,  1.53s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 28 avg_loss: 0.01707932046788224\n","EPOCH 29:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 29 avg_loss: 0.017290216176060295\n","EPOCH 30:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 118/118 [02:59\u003c00:00,  1.52s/it]\n"]},{"name":"stdout","output_type":"stream","text":["# epoch 30 avg_loss: 0.01707789805417849\n"]}],"source":["history = model.train(\n","    n_epoch=EPOCHS,\n","    p_uncond=P_UNCOND\n",")"]},{"cell_type":"markdown","metadata":{"id":"QK-c2N5tgKN7"},"source":["# Generated Image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_q_2fa4NhLEi"},"outputs":[],"source":["test_noise = torch.randn((1, 1, 32, 32)).to(device)\n","test_noise = test_noise.repeat(10, 1, 1, 1)\n","# test_noise: [10, 1, 32, 32]\n","\n","condition = torch.tensor(list(range(10))).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlf8YzvChDTb"},"outputs":[],"source":["test_steps = [10, 20, 50, 100, 200]\n","\n","for steps in test_steps:\n","    test_denoised_image = model.decoder.DDIM_sampling(\n","        test_noise,\n","        TIME_STEPS,\n","        c=condition,\n","        w=1,\n","        sampling_time_step=steps\n","    )\n","\n","    print_digits(test_denoised_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejBk-qibhgA8"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["m7z-5moymq7h"],"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}