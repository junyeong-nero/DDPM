{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBCygEM5hLEa",
        "outputId": "80416067-8db9-4640-f389-a960ad8fb16f"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -U -q torch torchvision scipy tdqm matplotlib scipy transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeVPd_CFqKVF"
      },
      "source": [
        "# Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayIRMJrwqNR2",
        "outputId": "06e32f7e-c1dd-436c-ac0f-ee8f416dd5bf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004v5ZfzxX6u"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikG5UIXCnvh6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hptABxxsmq7g"
      },
      "source": [
        "### Print Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJh_Om8QzgEY"
      },
      "outputs": [],
      "source": [
        "def image_normalize(image):\n",
        "    image = image.cpu()\n",
        "    n_channels = image.shape[0]\n",
        "    for channel in range(n_channels):\n",
        "        max_value = torch.max(image[channel])\n",
        "        min_value = torch.min(image[channel])\n",
        "        image[channel] = (image[channel] - min_value) / (max_value - min_value)\n",
        "\n",
        "    image = image.permute(1, 2, 0)\n",
        "\n",
        "    return image\n",
        "\n",
        "def print_image(image):\n",
        "    image = image_normalize(image)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "def print_2images(image1, image2):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(image_normalize(image1))\n",
        "    axes[0].set_title('Image 1')\n",
        "\n",
        "    axes[1].imshow(image_normalize(image2))\n",
        "    axes[1].set_title('Image 2')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_digits(result):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(10, 5))\n",
        "\n",
        "    B = result.shape[0]\n",
        "    for i in range(B):\n",
        "        axes[i].imshow(image_normalize(result[i]))\n",
        "        axes[i].set_title(i)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_result(result):\n",
        "    for original_image, noised_image, denoised_image in result:\n",
        "        batch_size = original_image.shape[0]\n",
        "        for idx in range(batch_size):\n",
        "            print_2images(original_image[idx], denoised_image[idx])\n",
        "            # print_image(image[idx])\n",
        "            # print_image(noised_image[idx])\n",
        "            # print_image(denoised_image[idx])\n",
        "\n",
        "\n",
        "def print_loss(loss_values):\n",
        "    epochs = list(range(1, len(loss_values) + 1))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, loss_values, 'b-o', label='Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Epoch vs Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0hElUKmq7g"
      },
      "source": [
        "### Torch Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV83NB-7-Ueo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7z-5moymq7h"
      },
      "source": [
        "### Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLjlFrOZmq7h",
        "outputId": "2150b2b9-ced7-4534-9189-1a3014b0fab6"
      },
      "outputs": [],
      "source": [
        "CLASSIFIER_MODEL = pipeline(\n",
        "    \"image-classification\",\n",
        "    model=\"farleyknight-org-username/vit-base-mnist\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def inception_ViT(inputs):\n",
        "\n",
        "    def convert_to_pil(x):\n",
        "        converted_images = []\n",
        "        for i in range(x.shape[0]):\n",
        "            converted_images.append(to_pil_image(x[i]))\n",
        "        return converted_images\n",
        "\n",
        "    def convert_classifier_results(results):\n",
        "        prob = [0.00000001] * 10\n",
        "        for result in results:\n",
        "            prob[int(result['label'])] = result['score']\n",
        "        return prob\n",
        "\n",
        "    # inputs : [B, 1, 32, 32]\n",
        "    out = CLASSIFIER_MODEL(convert_to_pil(inputs))\n",
        "    out = [convert_classifier_results(x) for x in out]\n",
        "    return out\n",
        "\n",
        "# Function to get inception features\n",
        "def get_inception_features(inception_model, result):\n",
        "    target, origin = [], []\n",
        "\n",
        "    for original_image, noised_image, denoised_image in result:\n",
        "        # denoised_image : [B, 1, 32, 32]\n",
        "        origin += inception_model(original_image)\n",
        "        target += inception_model(denoised_image)\n",
        "\n",
        "    return origin, target\n",
        "\n",
        "# Calculate FID\n",
        "def calculate_fid(origin, target):\n",
        "    mu1, sigma1 = np.mean(origin, axis=0), np.cov(origin, rowvar=False)\n",
        "    mu2, sigma2 = np.mean(target, axis=0), np.cov(target, rowvar=False)\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "def calculate_inception_score(results):\n",
        "    scores = []\n",
        "    for part in results:\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores.append(np.exp(np.mean([np.sum(p * np.log(p / py)) for p in part])))\n",
        "    return np.mean(scores), np.std(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWXFvNsVmq7h"
      },
      "outputs": [],
      "source": [
        "def print_scores(result):\n",
        "    origin, target = get_inception_features(inception_ViT, result)\n",
        "\n",
        "    origin_IS_mean, origin_IS_std = calculate_inception_score(origin)\n",
        "    print(f'[Origin] IS: {origin_IS_mean} ± {origin_IS_std}')\n",
        "\n",
        "    target_IS_mean, target_IS_std = calculate_inception_score(target)\n",
        "    print(f'[Target] IS: {target_IS_mean} ± {target_IS_std}')\n",
        "\n",
        "    FID = calculate_fid(origin, target)\n",
        "    print(f'FID: {FID}')\n",
        "\n",
        "    return origin_IS_mean, target_IS_mean, FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHS6FSDSn2hy"
      },
      "source": [
        "# Noise Scheduler\n",
        "- betas, alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq0_qYUenyDg"
      },
      "outputs": [],
      "source": [
        "class NoiseSchedule:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps,\n",
        "        beta_start = 0.0001,\n",
        "        beta_end = 0.02,\n",
        "        device = device,\n",
        "        init_type = \"linear\"\n",
        "    ) -> None:\n",
        "\n",
        "        self._size = n_timesteps\n",
        "        if init_type == \"linear\":\n",
        "            self._betas = torch.linspace(beta_start, beta_end, n_timesteps).to(device)\n",
        "        if init_type == \"exponential\":\n",
        "            self._betas = torch.from_numpy(np.geomspace(beta_start, beta_end, n_timesteps)).to(device)\n",
        "        self._alphas = self._calculate_alphas()\n",
        "\n",
        "    def _calculate_alphas(self):\n",
        "        self._alphas = torch.cumprod(1 - self._betas, axis=0)\n",
        "        return self._alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8PxHOV6n-ZV"
      },
      "source": [
        "# Forward Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHr0pKLWn5tI"
      },
      "outputs": [],
      "source": [
        "class ForwardEncoder:\n",
        "\n",
        "    def __init__(self, noise_schedule) -> None:\n",
        "        self.noise_schedule = noise_schedule\n",
        "\n",
        "    def noise(self, data, time_step):\n",
        "        # time_step : [B]\n",
        "        # data : [B, 1, 32, 32]\n",
        "\n",
        "        alpha = self.noise_schedule._alphas[time_step]\n",
        "        alpha = alpha.reshape(-1, 1, 1, 1)\n",
        "        # alpha : [B, 1, 1, 1]\n",
        "\n",
        "        epsilon = torch.randn(data.shape).to(device)\n",
        "        # torch.randn ~ N(0, 1)\n",
        "\n",
        "        return torch.sqrt(alpha) * data + torch.sqrt(1 - alpha) * epsilon, epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BIC7rsQ8en-"
      },
      "source": [
        "# Reverse Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV7lE5EZzyuy"
      },
      "outputs": [],
      "source": [
        "class ReverseDecoder:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        noise_schedule,\n",
        "        g\n",
        "    ) -> None:\n",
        "        self.noise_schedule = noise_schedule\n",
        "        self.g = g\n",
        "\n",
        "    def DDPM_sampling(\n",
        "        self,\n",
        "        noise_data,\n",
        "        time_step,\n",
        "        c = None,\n",
        "        w = 0\n",
        "    ):\n",
        "        # noise_data : [B, 1, 32, 32]\n",
        "        # c : [B]\n",
        "        # time_step : INT\n",
        "\n",
        "        batch_size = noise_data.shape[0]\n",
        "        # batch_size : B\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # step : [T - 1, T - 2, .. 2, 1, 0]\n",
        "            for step in range(time_step - 1, -1, -1):\n",
        "\n",
        "                t = torch.full((batch_size, ), step).to(device)\n",
        "                t = t.reshape(-1, 1, 1, 1)\n",
        "                # t : [B, 1, 1, 1]\n",
        "\n",
        "                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "                mu = 1 / torch.sqrt(1 - self.noise_schedule._betas[t]) * (noise_data - (self.noise_schedule._betas[t] / (1 - self.noise_schedule._alphas[t])) * predict_noise)\n",
        "                # mu : [B, 1, 32, 32]\n",
        "\n",
        "                if step == 0:\n",
        "                    # if t == 0, no add noise\n",
        "                    break\n",
        "\n",
        "                epsilon = torch.randn(noise_data.shape).to(device)\n",
        "                # epsilon : [B, 1, 32, 32]\n",
        "\n",
        "                noise_data = mu + torch.sqrt(self.noise_schedule._betas[t]) * epsilon\n",
        "                # noise_data : [B, 1, 32, 32]\n",
        "\n",
        "        return noise_data\n",
        "\n",
        "    def DDIM_sampling(\n",
        "        self,\n",
        "        noise_data,\n",
        "        time_step,\n",
        "        c = None,\n",
        "        w = 0,\n",
        "        sampling_time_step = 10,\n",
        "        custom_sampling_steps = None\n",
        "    ):\n",
        "        # noise_data : [B, 1, 32, 32]\n",
        "        # c : [B]\n",
        "        # time_step : INT\n",
        "\n",
        "        batch_size = noise_data.shape[0]\n",
        "        tau = list(range(0, time_step, time_step // sampling_time_step))\n",
        "        if custom_sampling_steps is not None:\n",
        "            tau = custom_sampling_steps\n",
        "\n",
        "        S = len(tau)\n",
        "\n",
        "        # batch_size : B\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # step : [T - 1, T - 2, .. 2, 1, 0]\n",
        "            for i in range(S - 1, -1, -1):\n",
        "\n",
        "                t = torch.full((batch_size, ), tau[i]).to(device)\n",
        "                t = t.reshape(-1, 1, 1, 1)\n",
        "                alpha_t = self.noise_schedule._alphas[t]\n",
        "\n",
        "                alpha_t_1 = torch.full((batch_size, 1, 1, 1,), 1).to(device)\n",
        "                if i - 1 >= 0:\n",
        "                    t_1 = torch.full((batch_size, ), tau[i - 1]).to(device)\n",
        "                    t_1 = t_1.reshape(-1, 1, 1, 1)\n",
        "                    alpha_t_1 = self.noise_schedule._alphas[t_1]\n",
        "\n",
        "                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "                first = torch.sqrt(alpha_t_1) * ((noise_data - torch.sqrt(1 - alpha_t) * predict_noise) / torch.sqrt(alpha_t))\n",
        "                second = torch.sqrt(1 - alpha_t_1) * predict_noise\n",
        "\n",
        "                noise_data = first + second\n",
        "\n",
        "        return noise_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0pbKzSh82Og"
      },
      "source": [
        "# UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG_ujnAqOf-H"
      },
      "source": [
        "### Backbones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSR1El6-Of-I"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, num_steps, time_emb_dim) -> None:\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        self.time_embed = nn.Embedding(num_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = self.sinusoidal_embedding(num_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "\n",
        "    def sinusoidal_embedding(self, n, d):\n",
        "        # Returns the standard positional embedding\n",
        "        embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n",
        "        sin_mask = torch.arange(0, n, 2)\n",
        "        embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
        "        embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.time_embed(input)\n",
        "\n",
        "\n",
        "class WideResNetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        n = 3,\n",
        "        kernel_size = 3,\n",
        "        stride = 1,\n",
        "        padding = 1,\n",
        "        num_groups = 32\n",
        "    ):\n",
        "        super(WideResNetBlock, self).__init__()\n",
        "        self.n = n\n",
        "        self.ks = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if kernel_size != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n",
        "                # nn.BatchNorm2d(out_size)\n",
        "            )\n",
        "\n",
        "        if is_batchnorm:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                    #  nn.BatchNorm2d(out_size),\n",
        "                    nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n",
        "                    nn.SiLU(inplace=True)\n",
        "                )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_channels = out_channels\n",
        "\n",
        "        else:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                    nn.SiLU(inplace=True)\n",
        "                )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_channels = out_channels\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for i in range(1, self.n + 1):\n",
        "            conv = getattr(self, 'conv%d' % i)\n",
        "            x = conv(x)\n",
        "        x += self.shortcut(inputs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        num_heads = 2,\n",
        "        num_groups = 32,\n",
        "    ):\n",
        "        super(MultiHeadAttentionBlock, self).__init__()\n",
        "\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        # For each of heads use d_k = d_v = d_model / num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = out_channels\n",
        "        self.d_keys = out_channels // num_heads\n",
        "        self.d_values = out_channels // num_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.W_K = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.W_V = nn.Linear(in_channels, out_channels, bias=False)\n",
        "\n",
        "        self.final_projection = nn.Linear(out_channels, out_channels, bias=False)\n",
        "        self.norm = nn.GroupNorm(num_channels=out_channels, num_groups=num_groups)\n",
        "\n",
        "    def split_features_for_heads(self, tensor):\n",
        "        batch, hw, emb_dim = tensor.shape\n",
        "        channels_per_head = emb_dim // self.num_heads\n",
        "        heads_splitted_tensor = torch.split(tensor, split_size_or_sections=channels_per_head, dim=-1)\n",
        "        heads_splitted_tensor = torch.stack(heads_splitted_tensor, 1)\n",
        "        return heads_splitted_tensor\n",
        "\n",
        "    def attention(self, q, k, v):\n",
        "\n",
        "        B, C, H, W = q.shape\n",
        "        q = q.view(B, C, q.shape[2] * q.shape[3]).transpose(1, 2)\n",
        "        k = k.view(B, C, k.shape[2] * k.shape[3]).transpose(1, 2)\n",
        "        v = v.view(B, C, v.shape[2] * v.shape[3]).transpose(1, 2)\n",
        "\n",
        "        # [B, H * W, C_in]\n",
        "\n",
        "        q = self.W_Q(q)\n",
        "        k = self.W_K(k)\n",
        "        v = self.W_V(v)\n",
        "        # N = H * W\n",
        "        # [B, N, C_out]\n",
        "\n",
        "        Q = self.split_features_for_heads(q)\n",
        "        K = self.split_features_for_heads(k)\n",
        "        V = self.split_features_for_heads(v)\n",
        "        # [B, num_heads, N, C_out / num_heads]\n",
        "\n",
        "        scale = self.d_keys ** -0.5\n",
        "        attention_scores = torch.softmax(torch.matmul(Q, K.transpose(-1, -2)) * scale, dim=-1)\n",
        "        attention_scores = torch.matmul(attention_scores, V)\n",
        "        # [B, num_heads, N, C_out / num_heads]\n",
        "\n",
        "        attention_scores = attention_scores.permute(0, 2, 1, 3).contiguous()\n",
        "        # [B, num_heads, N, C_out / num_heads] --> [B, N, num_heads, C_out / num_heads]\n",
        "\n",
        "        concatenated_heads_attention_scores = attention_scores.view(B, H * W, self.d_model)\n",
        "        # [B, N, num_heads, C_out / num_heads] --> [batch, N, C_out]\n",
        "\n",
        "        linear_projection = self.final_projection(concatenated_heads_attention_scores)\n",
        "        linear_projection = linear_projection.transpose(-1, -2).reshape(B, self.d_model, H, W)\n",
        "        # [B, N, C_out] -> [B, C_out, N] -> [B, C_out, H, W]\n",
        "\n",
        "        # Residual connection + norm\n",
        "        out = linear_projection\n",
        "        if self.is_batchnorm:\n",
        "            v = v.transpose(-1, -2).reshape(B, self.d_model, H, W)\n",
        "            out = self.norm(out + v)\n",
        "        return out\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        return self.attention(q, k, v)\n",
        "\n",
        "\n",
        "class SelfAttentionBlock(MultiHeadAttentionBlock):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        num_heads = 2,\n",
        "        num_groups = 32,\n",
        "    ):\n",
        "        super().__init__(in_channels, out_channels, num_heads, num_groups)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x, x, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eJz64ADOf-I"
      },
      "source": [
        "### UNet Body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olBnE6_v81Nf"
      },
      "outputs": [],
      "source": [
        "class UNetDown(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        base_model = WideResNetBlock,\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNetDown, self).__init__()\n",
        "        self.conv = base_model(in_channels, out_channels, is_batchnorm=is_batchnorm)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        base_model = WideResNetBlock,\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNetUp, self).__init__()\n",
        "        self.conv = base_model(out_channels * 2, out_channels, is_batchnorm=is_batchnorm)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "    def forward(self, inputs0, *input):\n",
        "        outputs0 = self.up(inputs0)\n",
        "        for i in range(len(input)):\n",
        "            outputs0 = torch.cat([outputs0, input[i]], 1)\n",
        "        return self.conv(outputs0)\n",
        "\n",
        "\n",
        "class UNetTimeEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out) -> None:\n",
        "        super(UNetTimeEmbedding, self).__init__()\n",
        "        self.ln = nn.Linear(dim_in, dim_out)\n",
        "        self.activation = nn.SiLU()\n",
        "        self.ln2 = nn.Linear(dim_out, dim_out)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B = inputs.shape[0]\n",
        "\n",
        "        x = self.ln(inputs)\n",
        "        x = self.activation(x)\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        return x.reshape(B, -1, 1, 1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels = 1,\n",
        "        out_channels = 1,\n",
        "        n_steps = 1000,\n",
        "        time_emb_dim = 256,\n",
        "        n_classes = 10,\n",
        "        class_emb_dim = 64,\n",
        "        channel_scale = 64,\n",
        "        num_channel_scale = 5,\n",
        "        custom_channel_scale = None,\n",
        "        cross_attention_layer_indices = [-1],\n",
        "        self_attention_layer_indices = [-1],\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNet, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "\n",
        "        self.self_attention_layer_indices = set(self_attention_layer_indices)\n",
        "        self.cross_attention_layer_indices = set(cross_attention_layer_indices)\n",
        "\n",
        "        # time embedding\n",
        "        self.time_embedding = PositionalEmbedding(n_steps, time_emb_dim)\n",
        "\n",
        "        # conditional variable embedding\n",
        "        self.context_embedding = PositionalEmbedding(n_classes, class_emb_dim)\n",
        "\n",
        "        if custom_channel_scale is None:\n",
        "            # channel exponenetial scales with `channel_scale`\n",
        "            # 64, 128, 256, 512, 1024\n",
        "            filters = [channel_scale * (2 ** i) for i in range(num_channel_scale)]\n",
        "        else:\n",
        "            # custom channel scales\n",
        "            num_channel_scale = len(custom_channel_scale)\n",
        "            filters = custom_channel_scale\n",
        "\n",
        "        self.num_layers = num_channel_scale\n",
        "\n",
        "        # Downsampling\n",
        "        filters.insert(0, in_channels)\n",
        "\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "            base_model = WideResNetBlock\n",
        "            if (layer_idx - self.num_layers) in self.self_attention_layer_indices:\n",
        "                base_model = SelfAttentionBlock\n",
        "\n",
        "            conv = UNetDown(in_channels=filters[layer_idx - 1],\n",
        "                            out_channels=filters[layer_idx],\n",
        "                            is_batchnorm=self.is_batchnorm,\n",
        "                            base_model=base_model)\n",
        "            temb = UNetTimeEmbedding(time_emb_dim, filters[layer_idx])\n",
        "            cemb = UNetTimeEmbedding(class_emb_dim, filters[layer_idx])\n",
        "            maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "            if (layer_idx - self.num_layers) in self.cross_attention_layer_indices:\n",
        "                cross_attention = MultiHeadAttentionBlock(\n",
        "                    in_channels=filters[layer_idx],\n",
        "                    out_channels=filters[layer_idx],\n",
        "                    is_batchnorm=False\n",
        "                )\n",
        "                setattr(self, 'down_cross_attention%d' % layer_idx, cross_attention)\n",
        "\n",
        "            setattr(self, 'down_conv%d' % layer_idx, conv)\n",
        "            setattr(self, 'down_temb%d' % layer_idx, temb)\n",
        "            setattr(self, 'down_cemb%d' % layer_idx, cemb)\n",
        "            setattr(self, 'down_maxpool%d' % layer_idx, maxpool)\n",
        "\n",
        "\n",
        "        # Bottleneck\n",
        "\n",
        "        self.center = UNetDown(filters[-2], filters[-1], is_batchnorm=self.is_batchnorm)\n",
        "        self.temb_center = UNetTimeEmbedding(time_emb_dim, filters[-1])\n",
        "        self.cemb_center = UNetTimeEmbedding(class_emb_dim, filters[-1])\n",
        "        self.cross_attention_center = MultiHeadAttentionBlock(\n",
        "            in_channels=filters[-1],\n",
        "            out_channels=filters[-1],\n",
        "            is_batchnorm=False\n",
        "        )\n",
        "\n",
        "        # upsampling\n",
        "        filters[0] = out_channels\n",
        "\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "            base_model = WideResNetBlock\n",
        "            if (layer_idx - self.num_layers) in self.self_attention_layer_indices:\n",
        "                base_model = SelfAttentionBlock\n",
        "            conv = UNetUp(filters[layer_idx + 1],\n",
        "                          filters[layer_idx],\n",
        "                          is_deconv=self.is_deconv,\n",
        "                          is_batchnorm=self.is_batchnorm,\n",
        "                          base_model=base_model)\n",
        "            temb = UNetTimeEmbedding(time_emb_dim, filters[layer_idx])\n",
        "\n",
        "            setattr(self, 'up_conv%d' % layer_idx, conv)\n",
        "            setattr(self, 'up_temb%d' % layer_idx, temb)\n",
        "\n",
        "        # output\n",
        "        self.outconv = nn.Conv2d(filters[1], self.out_channels, 3, padding=1)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        inputs,\n",
        "        t,\n",
        "        c = None\n",
        "    ):\n",
        "\n",
        "        t = self.time_embedding(t)\n",
        "        if c is not None:\n",
        "            c = self.context_embedding(c)\n",
        "\n",
        "        # inputs : [B, 1, 32, 32]\n",
        "\n",
        "        x = inputs\n",
        "        downsampling_result = [None]\n",
        "\n",
        "        # DOWN-SAMPLING\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "\n",
        "            conv = getattr(self, 'down_conv%d' % layer_idx)\n",
        "            temb = getattr(self, 'down_temb%d' % layer_idx)\n",
        "            cemb = getattr(self, 'down_cemb%d' % layer_idx)\n",
        "            maxpool = getattr(self, 'down_maxpool%d' % layer_idx)\n",
        "\n",
        "            x = conv(x)\n",
        "            downsampling_result.append(x)\n",
        "\n",
        "            if c is not None and (layer_idx - self.num_layers) in self.cross_attention_layer_indices:\n",
        "                CA = getattr(self, 'down_cross_attention%d' % layer_idx)\n",
        "                context_emb = cemb(c)\n",
        "                x = CA(x, context_emb, context_emb)\n",
        "\n",
        "            x += temb(t)\n",
        "            x = maxpool(x)\n",
        "\n",
        "        # BOTTLENECK\n",
        "\n",
        "        x = self.center(x)\n",
        "        if c is not None:\n",
        "            context_emb = self.cemb_center(c)\n",
        "            x = self.cross_attention_center(x, context_emb, context_emb)\n",
        "        x += self.temb_center(t)\n",
        "\n",
        "        # UP-SAMPLING\n",
        "\n",
        "        for layer_idx in range(self.num_layers - 1, 0, -1):\n",
        "            conv = getattr(self, 'up_conv%d' % layer_idx)\n",
        "            temb = getattr(self, 'up_temb%d' % layer_idx)\n",
        "            x = conv(x, downsampling_result[layer_idx])\n",
        "            x += temb(t)\n",
        "\n",
        "        return self.outconv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdKlF90S9eIC"
      },
      "source": [
        "# Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn5dfm7k9eiG"
      },
      "outputs": [],
      "source": [
        "class MyDiffusion:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps,\n",
        "        train_set = None,\n",
        "        test_set = None,\n",
        "        in_channels = 1,\n",
        "        out_channels = 1,\n",
        "        channel_scale = 64,\n",
        "        num_channle_scale = 5,\n",
        "        train_batch_size = 8,\n",
        "        test_batch_size = 8,\n",
        "        custom_channel_scale = None,\n",
        "        learning_rate = 0.0001\n",
        "    ):\n",
        "\n",
        "        self.n_timesteps = n_timesteps\n",
        "        self.channel_scale = channel_scale\n",
        "\n",
        "        # UNet for predicting total noise\n",
        "        self.g = UNet(in_channels=in_channels,\n",
        "                      out_channels=out_channels,\n",
        "                      n_steps=n_timesteps,\n",
        "                      channel_scale=channel_scale,\n",
        "                      num_channel_scale=num_channle_scale,\n",
        "                      custom_channel_scale=custom_channel_scale)\n",
        "        self.g = self.g.to(device)\n",
        "\n",
        "        # alpha, betas\n",
        "        self.noise_schedule = NoiseSchedule(n_timesteps=n_timesteps)\n",
        "\n",
        "        # forward encoder\n",
        "        self.encoder = ForwardEncoder(noise_schedule=self.noise_schedule)\n",
        "        self.decoder = ReverseDecoder(noise_schedule=self.noise_schedule, g=self.g)\n",
        "\n",
        "        # optimizer\n",
        "        self.lossFunction = torch.nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.g.parameters(), lr=learning_rate)\n",
        "\n",
        "        # datasets\n",
        "        if train_set:\n",
        "            self.training_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
        "        if test_set:\n",
        "            self.testing_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    def save(self, path='./model.pt'):\n",
        "        torch.save(self.g.state_dict(), path)\n",
        "\n",
        "\n",
        "    def load(self, path='./model.pt'):\n",
        "        self.g.load_state_dict(torch.load(path))\n",
        "        self.g.eval()\n",
        "\n",
        "\n",
        "    def train_one_epoch(\n",
        "        self,\n",
        "        n_iter_limit = None,\n",
        "        p_uncond = 0.1\n",
        "    ):\n",
        "\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, data in enumerate(tqdm(self.training_loader)):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs, label = data\n",
        "            inputs = inputs.to(device)\n",
        "            # print(inputs.shape)\n",
        "\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            # sampled timestep and conditional variables\n",
        "            t = torch.randint(0, self.n_timesteps, (batch_size, )).to(device)\n",
        "            c = label.to(device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image, epsilon = self.encoder.noise(inputs, t)\n",
        "\n",
        "            outputs = None\n",
        "            if torch.rand((1, )).item() < p_uncond:\n",
        "                outputs = self.g(noised_image, t)\n",
        "            else:\n",
        "                outputs = self.g(noised_image, t, c)\n",
        "\n",
        "            loss = self.lossFunction(outputs, epsilon)\n",
        "\n",
        "            # Adjust learning weights\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i == n_iter_limit:\n",
        "                break\n",
        "\n",
        "        return running_loss / len(self.training_loader)\n",
        "\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        n_epoch = 5,\n",
        "        n_iter_limit = None,\n",
        "        p_uncond = 0.1\n",
        "    ):\n",
        "\n",
        "        history = []\n",
        "\n",
        "        for epoch in range(n_epoch):\n",
        "            print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "            # Make sure gradient tracking is on, and do a pass over the data\n",
        "            self.g.train(True)\n",
        "            avg_loss = self.train_one_epoch(n_iter_limit=n_iter_limit,\n",
        "                                            p_uncond=p_uncond)\n",
        "            history.append(avg_loss)\n",
        "            print('# epoch {} avg_loss: {}'.format(epoch + 1, avg_loss))\n",
        "\n",
        "            model_path = 'U{}_T{}_E{}.pt'.format(self.channel_scale,\n",
        "                                                             self.n_timesteps,\n",
        "                                                             epoch + 1)\n",
        "            torch.save(self.g.state_dict(), model_path)\n",
        "            torch.save(torch.tensor(history), 'history.pt')\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        epochs = None,\n",
        "        sampling_type = \"DDPM\",\n",
        "        sampling_time_step = 10,\n",
        "        w = 0\n",
        "    ):\n",
        "        self.decoder.g = self.g\n",
        "        result = []\n",
        "        for i, data in enumerate(tqdm(self.testing_loader)):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs, label = data # data['image']\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            # timestep\n",
        "            t = torch.full((batch_size, ), self.n_timesteps - 1).to(device)\n",
        "            c = label.to(device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image, epsilon = self.encoder.noise(inputs, t)\n",
        "\n",
        "            # denoised image\n",
        "            denoised_image = None\n",
        "            if sampling_type == \"DDPM\":\n",
        "                denoised_image = self.decoder.DDPM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w\n",
        "                )\n",
        "            if sampling_type == \"DDIM\":\n",
        "                denoised_image = self.decoder.DDIM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w,\n",
        "                    sampling_time_step=sampling_time_step\n",
        "                )\n",
        "\n",
        "            result.append((inputs, noised_image, denoised_image))\n",
        "\n",
        "            if i == epochs - 1:\n",
        "                break\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_-bfyaE9-Zi"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyrjdgT3d_Yn"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 1000\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 30\n",
        "P_UNCOND = 0.1\n",
        "\n",
        "noise_schedule = NoiseSchedule(\n",
        "    n_timesteps=TIME_STEPS,\n",
        "    init_type=\"exponential\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBnmbCR7s054"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test = MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2iEoXPDpFXh"
      },
      "outputs": [],
      "source": [
        "model = MyDiffusion(\n",
        "    n_timesteps=TIME_STEPS,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    custom_channel_scale=[128, 128, 256, 256, 512, 512],\n",
        "    train_set=train,\n",
        "    test_set=test,\n",
        "    train_batch_size=BATCH_SIZE,\n",
        "    test_batch_size=8\n",
        ")\n",
        "\n",
        "# MODEL_PATH = '/content/drive/My Drive/models/DDPM_MNIST/UCA128_T1000_E30.pt'\n",
        "# model.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tq8fF7Vfs9fI",
        "outputId": "414e5d3d-8a48-4086-cd41-cb4977b2f3c4"
      },
      "outputs": [],
      "source": [
        "model.g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4lkZWaVo24g",
        "outputId": "0925d45a-3012-4efc-c363-3357c825491c"
      },
      "outputs": [],
      "source": [
        "print(\"model size : \", sum(p.numel() for p in model.g.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLwQ3_CdsgnQ",
        "outputId": "ed1b2658-9944-457a-f89b-9118f8115640"
      },
      "outputs": [],
      "source": [
        "history = model.train(\n",
        "    n_epoch=EPOCHS,\n",
        "    p_uncond=P_UNCOND\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK-c2N5tgKN7"
      },
      "source": [
        "# Generated Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q_2fa4NhLEi"
      },
      "outputs": [],
      "source": [
        "test_noise = torch.randn((1, 1, 32, 32)).to(device)\n",
        "test_noise = test_noise.repeat(10, 1, 1, 1)\n",
        "# test_noise: [10, 1, 32, 32]\n",
        "\n",
        "condition = torch.tensor(list(range(10))).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlf8YzvChDTb"
      },
      "outputs": [],
      "source": [
        "test_steps = [10, 20, 50, 100, 200]\n",
        "\n",
        "for steps in test_steps:\n",
        "    test_denoised_image = model.decoder.DDIM_sampling(\n",
        "        test_noise,\n",
        "        TIME_STEPS,\n",
        "        c=condition,\n",
        "        w=1,\n",
        "        sampling_time_step=steps\n",
        "    )\n",
        "\n",
        "    print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejBk-qibhgA8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "m7z-5moymq7h"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
