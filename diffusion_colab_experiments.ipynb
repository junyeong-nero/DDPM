{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBCygEM5hLEa",
        "outputId": "6a18a27d-6be8-4246-808e-27ace80ea853"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip\n",
        "!pip install -U -q torch torchvision scipy tdqm matplotlib scipy transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeVPd_CFqKVF"
      },
      "source": [
        "# Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayIRMJrwqNR2",
        "outputId": "b1f99602-2f96-404d-9f92-03d170e1af36"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004v5ZfzxX6u"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikG5UIXCnvh6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjb2VrnjfaKo"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)  # Python 기본 랜덤 시드 설정\n",
        "    np.random.seed(seed)  # NumPy 랜덤 시드 설정\n",
        "    torch.manual_seed(seed)  # PyTorch 랜덤 시드 설정\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)  # 모든 GPU의 시드 설정\n",
        "        torch.cuda.manual_seed_all(seed)  # 모든 GPU의 시드 설정\n",
        "\n",
        "    # CuDNN 사용 시 성능 향상을 위한 설정\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 시드 설정\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hptABxxsmq7g"
      },
      "source": [
        "### Print Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJh_Om8QzgEY"
      },
      "outputs": [],
      "source": [
        "def image_normalize(image):\n",
        "    image = image.cpu()\n",
        "    n_channels = image.shape[0]\n",
        "    for channel in range(n_channels):\n",
        "        max_value = torch.max(image[channel])\n",
        "        min_value = torch.min(image[channel])\n",
        "        image[channel] = (image[channel] - min_value) / (max_value - min_value)\n",
        "\n",
        "    image = image.permute(1, 2, 0)\n",
        "\n",
        "    return image\n",
        "\n",
        "def print_image(image):\n",
        "    image = image_normalize(image)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "def print_2images(image1, image2):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(image_normalize(image1))\n",
        "    axes[0].set_title('Image 1')\n",
        "\n",
        "    axes[1].imshow(image_normalize(image2))\n",
        "    axes[1].set_title('Image 2')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_digits(result):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(10, 5))\n",
        "\n",
        "    B = result.shape[0]\n",
        "    for i in range(B):\n",
        "        axes[i].imshow(image_normalize(result[i]))\n",
        "        axes[i].set_title(i)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_result(result):\n",
        "    for original_image, noised_image, denoised_image in result:\n",
        "        batch_size = original_image.shape[0]\n",
        "        for idx in range(batch_size):\n",
        "            print_2images(original_image[idx], denoised_image[idx])\n",
        "            # print_image(image[idx])\n",
        "            # print_image(noised_image[idx])\n",
        "            # print_image(denoised_image[idx])\n",
        "\n",
        "\n",
        "def print_seq(loss_values,\n",
        "               label='Training Loss',\n",
        "               x_label='Epoch',\n",
        "               y_label='Loss',\n",
        "               title='Loss vs. Epochs'):\n",
        "    epochs = list(range(1, len(loss_values) + 1))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, loss_values, 'b-o', label=label)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0hElUKmq7g"
      },
      "source": [
        "### Torch Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV83NB-7-Ueo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7z-5moymq7h"
      },
      "source": [
        "### Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "03dcf6e30386447c9d77cbbef20933a3",
            "3c3b074b11194ad78251c81cfa24c5a6",
            "2b35864dfa5f4f54b05b9ceb56aced02",
            "1a030f3f311c4b84aa5e993b1b15b069",
            "59a15d4dd32146099897b79a8220d465",
            "055d44172a0c451d91b90d0e06ecbe2d",
            "2f0a888db2884a3c8e356abea37a7444",
            "e6ddac863e47426884f56676e61e7a08",
            "68c3cc1a43a948549625aeb2fb3f6424",
            "a30e127545a549c98076234fe0464211",
            "3f93502fbb7c490094b1767d30286c6c",
            "f122bbefde264cc1af5c4d136862216f",
            "a5d90f88c0154f8aa1d7f84b6a30f2fd",
            "9d17e0d0c57e42deaa68a61415e98d57",
            "1abfe2a2083e4befbb208563e1da7b86",
            "81bfdb8ca94342e88071c5362b074a75",
            "7fb5c25b7f4a4733807ce4f95026b1f8",
            "e986abb58c3041d5bb0100fd2f240465",
            "6e30336a2a70409586c831ebce165041",
            "16b6711d184b4b3c863f3b8c27f92d03",
            "0d1578b5ad884d91ac9fec47e8dfd8af",
            "51c59b9a43ef4f2da0028a44f823437a",
            "aa8f2ee6fa2d4d27827af39c3d90de38",
            "98896cd4ef184ea28b4b49e7a3d63855",
            "6fb6dc0ba48e4b2c8071efd8f9477199",
            "c983d1d749cc422fa20d96e0972da286",
            "b4abf20e08bd41eba1f1f3c035368de7",
            "af305e43c47e4855ab0cbeae3e85006b",
            "e7820cdb7deb4cd4a6c7cb8abf44aea1",
            "1d65bed6ac0143dc827ea604fe4993fa",
            "651e53706ea945fb9d5e914b614c5f37",
            "f7f720f68d564639bbb3cfd40ce0fb0f",
            "b3b09170a65040ca850cbf7613174223"
          ]
        },
        "id": "FLjlFrOZmq7h",
        "outputId": "70394bb4-8565-4bbd-a8c4-6d6cac9e71ea"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "CLASSIFIER_MODEL = pipeline(\n",
        "    \"image-classification\",\n",
        "    model=\"farleyknight-org-username/vit-base-mnist\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def inception_ViT(inputs):\n",
        "\n",
        "    def convert_to_pil(x):\n",
        "        converted_images = []\n",
        "        for i in range(x.shape[0]):\n",
        "            converted_images.append(to_pil_image(x[i]))\n",
        "        return converted_images\n",
        "\n",
        "    def convert_classifier_results(results):\n",
        "        prob = [0.00000001] * 10\n",
        "        for result in results:\n",
        "            prob[int(result['label'])] = result['score']\n",
        "        return prob\n",
        "\n",
        "    # inputs : [B, 1, 32, 32]\n",
        "    out = CLASSIFIER_MODEL(convert_to_pil(inputs))\n",
        "    out = [convert_classifier_results(x) for x in out]\n",
        "    return out\n",
        "\n",
        "# Function to get inception features\n",
        "def get_inception_features(inception_model, result):\n",
        "    target, origin = [], []\n",
        "\n",
        "    for original_image, noised_image, denoised_image in result:\n",
        "        # denoised_image : [B, 1, 32, 32]\n",
        "        origin += inception_model(original_image)\n",
        "        target += inception_model(denoised_image)\n",
        "\n",
        "    return origin, target\n",
        "\n",
        "# Calculate FID\n",
        "def calculate_fid(origin, target):\n",
        "    mu1, sigma1 = np.mean(origin, axis=0), np.cov(origin, rowvar=False)\n",
        "    mu2, sigma2 = np.mean(target, axis=0), np.cov(target, rowvar=False)\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "def calculate_inception_score(results):\n",
        "    scores = []\n",
        "    for part in results:\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores.append(np.exp(np.mean([np.sum(p * np.log(p / py)) for p in part])))\n",
        "    return np.mean(scores), np.std(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWXFvNsVmq7h"
      },
      "outputs": [],
      "source": [
        "def print_scores(result):\n",
        "    origin, target = get_inception_features(inception_ViT, result)\n",
        "\n",
        "    origin_IS_mean, origin_IS_std = calculate_inception_score(origin)\n",
        "    print(f'[Origin] IS: {origin_IS_mean} ± {origin_IS_std}')\n",
        "\n",
        "    target_IS_mean, target_IS_std = calculate_inception_score(target)\n",
        "    print(f'[Target] IS: {target_IS_mean} ± {target_IS_std}')\n",
        "\n",
        "    FID = calculate_fid(origin, target)\n",
        "    print(f'FID: {FID}')\n",
        "\n",
        "    return origin_IS_mean, target_IS_mean, FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHS6FSDSn2hy"
      },
      "source": [
        "# Noise Scheduler\n",
        "- betas, alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq0_qYUenyDg"
      },
      "outputs": [],
      "source": [
        "class NoiseSchedule:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps,\n",
        "        beta_start = 0.0001,\n",
        "        beta_end = 0.02,\n",
        "        init_type = \"linear\",\n",
        "        device = device\n",
        "    ) -> None:\n",
        "\n",
        "        self._size = n_timesteps\n",
        "        if init_type == \"linear\":\n",
        "            self._betas = torch.linspace(beta_start, beta_end, n_timesteps).to(device)\n",
        "        if init_type == \"exponential\":\n",
        "            self._betas = torch.from_numpy(np.geomspace(beta_start, beta_end, n_timesteps)).to(device)\n",
        "        self._alphas = self._calculate_alphas()\n",
        "\n",
        "    def _calculate_alphas(self):\n",
        "        self._alphas = torch.cumprod(1 - self._betas, axis=0)\n",
        "        return self._alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8PxHOV6n-ZV"
      },
      "source": [
        "# Forward Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHr0pKLWn5tI"
      },
      "outputs": [],
      "source": [
        "class ForwardEncoder:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        noise_schedule,\n",
        "        device = None\n",
        "    ):\n",
        "        self.noise_schedule = noise_schedule\n",
        "        self.device = device\n",
        "\n",
        "    def noise(self, data, time_step):\n",
        "        # time_step : [B]\n",
        "        # data : [B, 1, 32, 32]\n",
        "\n",
        "        alpha = self.noise_schedule._alphas[time_step]\n",
        "        alpha = alpha.reshape(-1, 1, 1, 1)\n",
        "        # alpha : [B, 1, 1, 1]\n",
        "\n",
        "        epsilon = torch.randn(data.shape).to(self.device)\n",
        "        # torch.randn ~ N(0, 1)\n",
        "\n",
        "        return torch.sqrt(alpha) * data + torch.sqrt(1 - alpha) * epsilon, epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BIC7rsQ8en-"
      },
      "source": [
        "# Reverse Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV7lE5EZzyuy"
      },
      "outputs": [],
      "source": [
        "class ReverseDecoder:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        noise_schedule,\n",
        "        g,\n",
        "        device = None\n",
        "    ):\n",
        "        self.noise_schedule = noise_schedule\n",
        "        self.device = device\n",
        "        self.g = g\n",
        "\n",
        "    def DDPM_sampling(\n",
        "        self,\n",
        "        noise_data,\n",
        "        time_step,\n",
        "        c = None,\n",
        "        w = 0\n",
        "    ):\n",
        "        # noise_data : [B, 1, 32, 32]\n",
        "        # c : [B]\n",
        "        # time_step : INT\n",
        "\n",
        "        origin_data = noise_data.clone()\n",
        "        batch_size = noise_data.shape[0]\n",
        "        # batch_size : B\n",
        "\n",
        "        history_with_origin = []\n",
        "        history_with_prev = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # step : [T - 1, T - 2, .. 2, 1, 0]\n",
        "            for step in range(time_step - 1, -1, -1):\n",
        "\n",
        "                t = torch.full((batch_size, ), step).to(self.device)\n",
        "                t = t.reshape(-1, 1, 1, 1)\n",
        "                # t : [B, 1, 1, 1]\n",
        "\n",
        "                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "                mu = 1 / torch.sqrt(1 - self.noise_schedule._betas[t]) * (noise_data - (self.noise_schedule._betas[t] / (1 - self.noise_schedule._alphas[t])) * predict_noise)\n",
        "                # mu : [B, 1, 32, 32]\n",
        "\n",
        "                if step == 0:\n",
        "                    # if t == 0, no add noise\n",
        "                    break\n",
        "\n",
        "                epsilon = torch.randn(noise_data.shape).to(self.device)\n",
        "                new_data = mu + torch.sqrt(self.noise_schedule._betas[t]) * epsilon\n",
        "\n",
        "                history_with_origin.append(torch.norm(origin_data - noise_data))\n",
        "                history_with_prev.append(torch.norm(new_data - noise_data))\n",
        "                noise_data = new_data\n",
        "\n",
        "        torch.save(torch.tensor(history_with_origin), \"DDPM_origin.pt\")\n",
        "        torch.save(torch.tensor(history_with_prev), \"DDPM_prev.pt\")\n",
        "\n",
        "        return noise_data\n",
        "\n",
        "    def DDIM_sampling(\n",
        "        self,\n",
        "        noise_data,\n",
        "        time_step,\n",
        "        c = None,\n",
        "        w = 0,\n",
        "        sampling_steps = 10,\n",
        "        sampling_types = \"linear\",\n",
        "        custom_sampling_steps = None\n",
        "    ):\n",
        "        # noise_data : [B, 1, 32, 32]\n",
        "        # c : [B]\n",
        "        # time_step : INT\n",
        "\n",
        "        B = noise_data.shape[0]\n",
        "        tau = None\n",
        "\n",
        "        if sampling_types == \"linear\":\n",
        "            tau = list(range(0, time_step, time_step // sampling_steps))\n",
        "        if sampling_types == \"exponential\":\n",
        "            tau = list(np.geomspace(1, time_step, time_step // sampling_steps))\n",
        "\n",
        "        if custom_sampling_steps is not None:\n",
        "            tau = custom_sampling_steps\n",
        "\n",
        "        S = len(tau)\n",
        "\n",
        "        origin_data = noise_data.clone()\n",
        "        history_with_origin = []\n",
        "        history_with_prev = []\n",
        "\n",
        "        # batch_size : B\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # step : [T - 1, T - 2, .. 2, 1, 0]\n",
        "            for i in range(S - 1, -1, -1):\n",
        "\n",
        "                t = torch.full((B, ), tau[i]).to(self.device)\n",
        "                t = t.reshape(-1, 1, 1, 1)\n",
        "                alpha_t = self.noise_schedule._alphas[t]\n",
        "\n",
        "                alpha_t_1 = torch.full((B, 1, 1, 1,), 1).to(self.device)\n",
        "                if i - 1 >= 0:\n",
        "                    t_1 = torch.full((B, ), tau[i - 1]).to(self.device)\n",
        "                    t_1 = t_1.reshape(-1, 1, 1, 1)\n",
        "                    alpha_t_1 = self.noise_schedule._alphas[t_1]\n",
        "\n",
        "                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "                new_data = torch.sqrt(alpha_t_1) * ((noise_data - torch.sqrt(1 - alpha_t) * predict_noise) / torch.sqrt(alpha_t)) + torch.sqrt(1 - alpha_t_1) * predict_noise\n",
        "\n",
        "                history_with_origin.append(torch.norm(origin_data - noise_data))\n",
        "                history_with_prev.append(torch.norm(new_data - noise_data))\n",
        "                noise_data = new_data\n",
        "\n",
        "        torch.save(torch.tensor(history_with_origin), \"DDIM_origin.pt\")\n",
        "        torch.save(torch.tensor(history_with_prev), \"DDIM_prev.pt\")\n",
        "        return noise_data\n",
        "\n",
        "\n",
        "    def DDIM_sampling_step(\n",
        "        self,\n",
        "        noise_data,\n",
        "        t,\n",
        "        predict_noise = None,\n",
        "        c = None,\n",
        "        w = 1,\n",
        "        t_1 = None\n",
        "    ):\n",
        "\n",
        "        t = t.reshape(-1, 1, 1, 1)\n",
        "        if t_1 is None:\n",
        "            t_1 = torch.clamp(t - 1, min=0)\n",
        "\n",
        "        alpha_t = self.noise_schedule._alphas[t]\n",
        "        alpha_t_1 = self.noise_schedule._alphas[t_1]\n",
        "\n",
        "        if predict_noise is None:\n",
        "            predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "        V1 = torch.sqrt(alpha_t_1) * ((noise_data - torch.sqrt(1 - alpha_t) * predict_noise) / torch.sqrt(alpha_t))\n",
        "        V2 = torch.sqrt(1 - alpha_t_1) * predict_noise\n",
        "\n",
        "        return V1 + V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0pbKzSh82Og"
      },
      "source": [
        "# UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG_ujnAqOf-H"
      },
      "source": [
        "### Backbones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSR1El6-Of-I"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, num_steps, time_emb_dim) -> None:\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        self.time_embed = nn.Embedding(num_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = self.sinusoidal_embedding(num_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "\n",
        "    def sinusoidal_embedding(self, n, d):\n",
        "        # Returns the standard positional embedding\n",
        "        embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n",
        "        sin_mask = torch.arange(0, n, 2)\n",
        "        embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
        "        embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.time_embed(input)\n",
        "\n",
        "\n",
        "class WideResNetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        n = 3,\n",
        "        kernel_size = 3,\n",
        "        stride = 1,\n",
        "        padding = 1,\n",
        "        num_groups = 32\n",
        "    ):\n",
        "        super(WideResNetBlock, self).__init__()\n",
        "        self.n = n\n",
        "        self.ks = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if kernel_size != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n",
        "                # nn.BatchNorm2d(out_size)\n",
        "            )\n",
        "\n",
        "        if is_batchnorm:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                    #  nn.BatchNorm2d(out_size),\n",
        "                    nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n",
        "                    nn.SiLU(inplace=True)\n",
        "                )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_channels = out_channels\n",
        "\n",
        "        else:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                    nn.SiLU(inplace=True)\n",
        "                )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_channels = out_channels\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for i in range(1, self.n + 1):\n",
        "            conv = getattr(self, 'conv%d' % i)\n",
        "            x = conv(x)\n",
        "        x += self.shortcut(inputs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        num_heads = 2,\n",
        "        num_groups = 32,\n",
        "    ):\n",
        "        super(MultiHeadAttentionBlock, self).__init__()\n",
        "\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        # For each of heads use d_k = d_v = d_model / num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = out_channels\n",
        "        self.d_keys = out_channels // num_heads\n",
        "        self.d_values = out_channels // num_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.W_K = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.W_V = nn.Linear(in_channels, out_channels, bias=False)\n",
        "\n",
        "        self.final_projection = nn.Linear(out_channels, out_channels, bias=False)\n",
        "        self.norm = nn.GroupNorm(num_channels=out_channels, num_groups=num_groups)\n",
        "\n",
        "    def split_features_for_heads(self, tensor):\n",
        "        batch, hw, emb_dim = tensor.shape\n",
        "        channels_per_head = emb_dim // self.num_heads\n",
        "        heads_splitted_tensor = torch.split(tensor, split_size_or_sections=channels_per_head, dim=-1)\n",
        "        heads_splitted_tensor = torch.stack(heads_splitted_tensor, 1)\n",
        "        return heads_splitted_tensor\n",
        "\n",
        "    def attention(self, q, k, v):\n",
        "\n",
        "        B, C, H, W = q.shape\n",
        "        q = q.view(B, C, q.shape[2] * q.shape[3]).transpose(1, 2)\n",
        "        k = k.view(B, C, k.shape[2] * k.shape[3]).transpose(1, 2)\n",
        "        v = v.view(B, C, v.shape[2] * v.shape[3]).transpose(1, 2)\n",
        "\n",
        "        # [B, H * W, C_in]\n",
        "\n",
        "        q = self.W_Q(q)\n",
        "        k = self.W_K(k)\n",
        "        v = self.W_V(v)\n",
        "        # N = H * W\n",
        "        # [B, N, C_out]\n",
        "\n",
        "        Q = self.split_features_for_heads(q)\n",
        "        K = self.split_features_for_heads(k)\n",
        "        V = self.split_features_for_heads(v)\n",
        "        # [B, num_heads, N, C_out / num_heads]\n",
        "\n",
        "        scale = self.d_keys ** -0.5\n",
        "        attention_scores = torch.softmax(torch.matmul(Q, K.transpose(-1, -2)) * scale, dim=-1)\n",
        "        attention_scores = torch.matmul(attention_scores, V)\n",
        "        # [B, num_heads, N, C_out / num_heads]\n",
        "\n",
        "        attention_scores = attention_scores.permute(0, 2, 1, 3).contiguous()\n",
        "        # [B, num_heads, N, C_out / num_heads] --> [B, N, num_heads, C_out / num_heads]\n",
        "\n",
        "        concatenated_heads_attention_scores = attention_scores.view(B, H * W, self.d_model)\n",
        "        # [B, N, num_heads, C_out / num_heads] --> [batch, N, C_out]\n",
        "\n",
        "        linear_projection = self.final_projection(concatenated_heads_attention_scores)\n",
        "        linear_projection = linear_projection.transpose(-1, -2).reshape(B, self.d_model, H, W)\n",
        "        # [B, N, C_out] -> [B, C_out, N] -> [B, C_out, H, W]\n",
        "\n",
        "        # Residual connection + norm\n",
        "        out = linear_projection\n",
        "        if self.is_batchnorm:\n",
        "            v = v.transpose(-1, -2).reshape(B, self.d_model, H, W)\n",
        "            out = self.norm(out + v)\n",
        "        return out\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        return self.attention(q, k, v)\n",
        "\n",
        "\n",
        "class SelfAttentionBlock(MultiHeadAttentionBlock):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        num_heads = 2,\n",
        "        num_groups = 32,\n",
        "    ):\n",
        "        super().__init__(in_channels, out_channels, num_heads, num_groups)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x, x, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eJz64ADOf-I"
      },
      "source": [
        "### UNet Body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olBnE6_v81Nf"
      },
      "outputs": [],
      "source": [
        "class UNetDown(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        base_model = WideResNetBlock,\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNetDown, self).__init__()\n",
        "        self.conv = base_model(in_channels, out_channels, is_batchnorm=is_batchnorm)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        base_model = WideResNetBlock,\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNetUp, self).__init__()\n",
        "        self.conv = base_model(out_channels * 2, out_channels, is_batchnorm=is_batchnorm)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "    def forward(self, inputs0, *input):\n",
        "        outputs0 = self.up(inputs0)\n",
        "        for i in range(len(input)):\n",
        "            outputs0 = torch.cat([outputs0, input[i]], 1)\n",
        "        return self.conv(outputs0)\n",
        "\n",
        "\n",
        "class UNetTimeEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out) -> None:\n",
        "        super(UNetTimeEmbedding, self).__init__()\n",
        "        self.ln = nn.Linear(dim_in, dim_out)\n",
        "        self.activation = nn.SiLU()\n",
        "        self.ln2 = nn.Linear(dim_out, dim_out)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B = inputs.shape[0]\n",
        "\n",
        "        x = self.ln(inputs)\n",
        "        x = self.activation(x)\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        return x.reshape(B, -1, 1, 1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels = 1,\n",
        "        out_channels = 1,\n",
        "        n_steps = 1000,\n",
        "        time_emb_dim = 256,\n",
        "        n_classes = 10,\n",
        "        class_emb_dim = 64,\n",
        "        channel_scale = 64,\n",
        "        num_channel_scale = 5,\n",
        "        custom_channel_scale = None,\n",
        "        cross_attention_layer_indices = [-1],\n",
        "        self_attention_layer_indices = [-1],\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNet, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "\n",
        "        self.self_attention_layer_indices = set(self_attention_layer_indices)\n",
        "        self.cross_attention_layer_indices = set(cross_attention_layer_indices)\n",
        "\n",
        "        # time embedding\n",
        "        self.time_embedding = PositionalEmbedding(n_steps, time_emb_dim)\n",
        "\n",
        "        # conditional variable embedding\n",
        "        self.context_embedding = PositionalEmbedding(n_classes, class_emb_dim)\n",
        "\n",
        "        if custom_channel_scale is None:\n",
        "            # channel exponenetial scales with `channel_scale`\n",
        "            # 64, 128, 256, 512, 1024\n",
        "            filters = [channel_scale * (2 ** i) for i in range(num_channel_scale)]\n",
        "        else:\n",
        "            # custom channel scales\n",
        "            num_channel_scale = len(custom_channel_scale)\n",
        "            filters = custom_channel_scale\n",
        "\n",
        "        self.num_layers = num_channel_scale\n",
        "\n",
        "        # Downsampling\n",
        "        filters.insert(0, in_channels)\n",
        "\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "            base_model = WideResNetBlock\n",
        "            if (layer_idx - self.num_layers) in self.self_attention_layer_indices:\n",
        "                base_model = SelfAttentionBlock\n",
        "\n",
        "            conv = UNetDown(in_channels=filters[layer_idx - 1],\n",
        "                            out_channels=filters[layer_idx],\n",
        "                            is_batchnorm=self.is_batchnorm,\n",
        "                            base_model=base_model)\n",
        "            temb = UNetTimeEmbedding(time_emb_dim, filters[layer_idx])\n",
        "            cemb = UNetTimeEmbedding(class_emb_dim, filters[layer_idx])\n",
        "            maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "            if (layer_idx - self.num_layers) in self.cross_attention_layer_indices:\n",
        "                cross_attention = MultiHeadAttentionBlock(\n",
        "                    in_channels=filters[layer_idx],\n",
        "                    out_channels=filters[layer_idx],\n",
        "                    is_batchnorm=False\n",
        "                )\n",
        "                setattr(self, 'down_cross_attention%d' % layer_idx, cross_attention)\n",
        "\n",
        "            setattr(self, 'down_conv%d' % layer_idx, conv)\n",
        "            setattr(self, 'down_temb%d' % layer_idx, temb)\n",
        "            setattr(self, 'down_cemb%d' % layer_idx, cemb)\n",
        "            setattr(self, 'down_maxpool%d' % layer_idx, maxpool)\n",
        "\n",
        "\n",
        "        # Bottleneck\n",
        "\n",
        "        self.center = UNetDown(filters[-2], filters[-1], is_batchnorm=self.is_batchnorm)\n",
        "        self.temb_center = UNetTimeEmbedding(time_emb_dim, filters[-1])\n",
        "        self.cemb_center = UNetTimeEmbedding(class_emb_dim, filters[-1])\n",
        "        self.cross_attention_center = MultiHeadAttentionBlock(\n",
        "            in_channels=filters[-1],\n",
        "            out_channels=filters[-1],\n",
        "            is_batchnorm=False\n",
        "        )\n",
        "\n",
        "        # upsampling\n",
        "        filters[0] = out_channels\n",
        "\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "            base_model = WideResNetBlock\n",
        "            if (layer_idx - self.num_layers) in self.self_attention_layer_indices:\n",
        "                base_model = SelfAttentionBlock\n",
        "            conv = UNetUp(filters[layer_idx + 1],\n",
        "                          filters[layer_idx],\n",
        "                          is_deconv=self.is_deconv,\n",
        "                          is_batchnorm=self.is_batchnorm,\n",
        "                          base_model=base_model)\n",
        "            temb = UNetTimeEmbedding(time_emb_dim, filters[layer_idx])\n",
        "\n",
        "            setattr(self, 'up_conv%d' % layer_idx, conv)\n",
        "            setattr(self, 'up_temb%d' % layer_idx, temb)\n",
        "\n",
        "        # output\n",
        "        self.outconv = nn.Conv2d(filters[1], self.out_channels, 3, padding=1)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        inputs,\n",
        "        t,\n",
        "        c = None\n",
        "    ):\n",
        "\n",
        "        t = self.time_embedding(t)\n",
        "        if c is not None:\n",
        "            c = self.context_embedding(c)\n",
        "\n",
        "        # inputs : [B, 1, 32, 32]\n",
        "\n",
        "        x = inputs\n",
        "        downsampling_result = [None]\n",
        "\n",
        "        # DOWN-SAMPLING\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "\n",
        "            conv = getattr(self, 'down_conv%d' % layer_idx)\n",
        "            temb = getattr(self, 'down_temb%d' % layer_idx)\n",
        "            cemb = getattr(self, 'down_cemb%d' % layer_idx)\n",
        "            maxpool = getattr(self, 'down_maxpool%d' % layer_idx)\n",
        "\n",
        "            x = conv(x)\n",
        "            downsampling_result.append(x)\n",
        "\n",
        "            if c is not None and (layer_idx - self.num_layers) in self.cross_attention_layer_indices:\n",
        "                CA = getattr(self, 'down_cross_attention%d' % layer_idx)\n",
        "                context_emb = cemb(c)\n",
        "                x = CA(x, context_emb, context_emb)\n",
        "\n",
        "            x += temb(t)\n",
        "            x = maxpool(x)\n",
        "\n",
        "        # BOTTLENECK\n",
        "\n",
        "        x = self.center(x)\n",
        "        if c is not None:\n",
        "            context_emb = self.cemb_center(c)\n",
        "            x = self.cross_attention_center(x, context_emb, context_emb)\n",
        "        x += self.temb_center(t)\n",
        "\n",
        "        # UP-SAMPLING\n",
        "\n",
        "        for layer_idx in range(self.num_layers - 1, 0, -1):\n",
        "            conv = getattr(self, 'up_conv%d' % layer_idx)\n",
        "            temb = getattr(self, 'up_temb%d' % layer_idx)\n",
        "            x = conv(x, downsampling_result[layer_idx])\n",
        "            x += temb(t)\n",
        "\n",
        "        return self.outconv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2PBHGeZVcqt"
      },
      "source": [
        "# Sampling Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_TOLF-GVcqt"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps\n",
        "    ):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(1, n_timesteps, bias=False)\n",
        "        torch.nn.init.ones_(self.linear.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class SamplingWeights():\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps,\n",
        "        learning_rate = 10e-4,\n",
        "    ) -> None:\n",
        "\n",
        "        self.n_timesteps = n_timesteps\n",
        "        self.weights = LinearRegressionModel(\n",
        "            n_timesteps=n_timesteps\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.weights.parameters(), lr=learning_rate)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights.linear.weight.clone().detach()\n",
        "\n",
        "    def result_convert(self, timestep, loss_item):\n",
        "        B = timestep.shape[0]\n",
        "        result = self.get_weights().unsqueeze(0).repeat((B, 1, 1))\n",
        "        for i in range(B):\n",
        "            result[i][timestep[i]] = loss_item[i]\n",
        "\n",
        "        # print(result.shape)\n",
        "        return result\n",
        "\n",
        "    def train_one_epoch(self, timestep, loss_item):\n",
        "        B = timestep.shape[0]\n",
        "        y = self.result_convert(timestep, loss_item)\n",
        "\n",
        "        for i in range(B):\n",
        "            outputs = self.weights.linear.weight\n",
        "            # print(outputs.shape, y[i].shape)\n",
        "            loss = self.criterion(outputs, y[i])\n",
        "\n",
        "            # 기울기 초기화, 역전파, 옵티마이저 스텝\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdKlF90S9eIC"
      },
      "source": [
        "# Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn5dfm7k9eiG"
      },
      "outputs": [],
      "source": [
        "class MyDiffusion:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps,\n",
        "        train_set = None,\n",
        "        test_set = None,\n",
        "        in_channels = 1,\n",
        "        out_channels = 1,\n",
        "        channel_scale = 64,\n",
        "        num_channle_scale = 5,\n",
        "        train_batch_size = 8,\n",
        "        test_batch_size = 8,\n",
        "        custom_channel_scale = None,\n",
        "        learning_rate = 0.0001,\n",
        "        device = None\n",
        "    ):\n",
        "\n",
        "        self.n_timesteps = n_timesteps\n",
        "        self.channel_scale = channel_scale\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # UNet for predicting total noise\n",
        "        self.g = UNet(in_channels=in_channels,\n",
        "                      out_channels=out_channels,\n",
        "                      n_steps=n_timesteps,\n",
        "                      channel_scale=channel_scale,\n",
        "                      num_channel_scale=num_channle_scale,\n",
        "                      custom_channel_scale=custom_channel_scale)\n",
        "\n",
        "        self.g = self.g.to(device)\n",
        "\n",
        "        # Sampling Weights\n",
        "        self.sampling_weights = SamplingWeights(\n",
        "            n_timesteps=n_timesteps\n",
        "        )\n",
        "\n",
        "        # Noise Scheduler\n",
        "        self.noise_schedule = NoiseSchedule(\n",
        "            n_timesteps=n_timesteps,\n",
        "            device=device\n",
        "            # init_type=\"exponential\",\n",
        "        )\n",
        "\n",
        "        # Forward Encoder\n",
        "        self.encoder = ForwardEncoder(\n",
        "            noise_schedule=self.noise_schedule,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Reverse Decoder\n",
        "        self.decoder = ReverseDecoder(\n",
        "            noise_schedule=self.noise_schedule,\n",
        "            g=self.g,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Optimizer\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.g.parameters(), lr=learning_rate)\n",
        "\n",
        "        # DataLoader\n",
        "        if train_set:\n",
        "            self.training_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
        "        if test_set:\n",
        "            self.testing_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    def save(self, path='./model.pt'):\n",
        "        torch.save(self.g.state_dict(), path)\n",
        "\n",
        "\n",
        "    def load(self, path='./model.pt'):\n",
        "        self.g.load_state_dict(torch.load(path))\n",
        "        self.g.eval()\n",
        "\n",
        "\n",
        "    def train_one_epoch(\n",
        "        self,\n",
        "        p_uncond = 0.1,\n",
        "        w = 1\n",
        "    ):\n",
        "\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, data in enumerate(tqdm(self.training_loader)):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs, label = data\n",
        "            inputs = inputs.to(self.device)\n",
        "            # print(inputs.shape)\n",
        "\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            # sampled timestep and conditional variables\n",
        "            t = torch.randint(0, self.n_timesteps, (batch_size, )).to(self.device)\n",
        "            c = label.to(self.device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image, epsilon = self.encoder.noise(inputs, t)\n",
        "\n",
        "            # training diffusion model\n",
        "            outputs = None\n",
        "            if torch.rand((1, )).item() < p_uncond:\n",
        "                outputs = self.g(noised_image, t)\n",
        "            else:\n",
        "                outputs = self.g(noised_image, t, c)\n",
        "\n",
        "            loss = self.criterion(outputs, epsilon)\n",
        "\n",
        "            # Adjust learning weights\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            self.optimizer.step()\n",
        "\n",
        "\n",
        "            # training sampling weight\n",
        "            outputs = outputs.detach().clone()\n",
        "            sampling_loss = self.decoder.DDIM_sampling_step(\n",
        "                noise_data=noised_image,\n",
        "                predict_noise=outputs,\n",
        "                t=t,\n",
        "                c=c,\n",
        "                w=w\n",
        "            )\n",
        "\n",
        "            sampling_loss = torch.linalg.matrix_norm(sampling_loss - noised_image)\n",
        "            self.sampling_weights.train_one_epoch(t, sampling_loss)\n",
        "\n",
        "            # update loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "        return running_loss / len(self.training_loader)\n",
        "\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        n_epoch = 5,\n",
        "        n_iter_limit = None,\n",
        "        p_uncond = 0.1\n",
        "    ):\n",
        "\n",
        "        history = []\n",
        "\n",
        "        for epoch in range(n_epoch):\n",
        "            self.g.train(True)\n",
        "            print('EPOCH {}:'.format(epoch + 1))\n",
        "            avg_loss = self.train_one_epoch(p_uncond=p_uncond)\n",
        "            history.append(avg_loss)\n",
        "            print('# epoch {} avg_loss: {}'.format(epoch + 1, avg_loss))\n",
        "\n",
        "\n",
        "            model_path = 'U{}_T{}_E{}.pt'.format(\n",
        "                self.channel_scale,\n",
        "                self.n_timesteps,\n",
        "                epoch + 1\n",
        "            )\n",
        "\n",
        "            torch.save(self.g.state_dict(), model_path)\n",
        "            torch.save(torch.tensor(history), 'history.pt')\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        epochs = None,\n",
        "        sampling_type = \"DDPM\",\n",
        "        sampling_time_step = 10,\n",
        "        custom_sampling_steps = None,\n",
        "        w = 0\n",
        "    ):\n",
        "        self.decoder.g = self.g\n",
        "        result = []\n",
        "        for i, data in enumerate(tqdm(self.testing_loader)):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs, label = data # data['image']\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            # timestep\n",
        "            t = torch.full((batch_size, ), self.n_timesteps - 1).to(self.device)\n",
        "            c = label.to(self.device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image, epsilon = self.encoder.noise(inputs, t)\n",
        "\n",
        "            # denoised image\n",
        "            denoised_image = None\n",
        "            if sampling_type == \"DDPM\":\n",
        "                denoised_image = self.decoder.DDPM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w\n",
        "                )\n",
        "            if sampling_type == \"DDIM\":\n",
        "                denoised_image = self.decoder.DDIM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w,\n",
        "                    sampling_steps=sampling_time_step,\n",
        "                    custom_sampling_steps=custom_sampling_steps\n",
        "                )\n",
        "\n",
        "            result.append((inputs, noised_image, denoised_image))\n",
        "\n",
        "            if i == epochs - 1:\n",
        "                break\n",
        "\n",
        "        return result\n",
        "\n",
        "    def evaluate_with_noise(\n",
        "        self,\n",
        "        epochs = 10,\n",
        "        sampling_type = \"DDPM\",\n",
        "        sampling_time_step = 10,\n",
        "        custom_sampling_steps = None,\n",
        "        w = 0\n",
        "    ):\n",
        "        self.decoder.g = self.g\n",
        "\n",
        "        B = self.test_batch_size\n",
        "        result = []\n",
        "        for i in range(epochs):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs = torch.randn((B, 1, 32, 32)).to(self.device)\n",
        "\n",
        "            # timestep and context\n",
        "            t = torch.full((B, ), self.n_timesteps - 1).to(self.device)\n",
        "            c = torch.randint(0, 10, (B, )).to(self.device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image = inputs\n",
        "\n",
        "            # denoised image\n",
        "            denoised_image = None\n",
        "            if sampling_type == \"DDPM\":\n",
        "                denoised_image = self.decoder.DDPM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w\n",
        "                )\n",
        "            if sampling_type == \"DDIM\":\n",
        "                denoised_image = self.decoder.DDIM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w,\n",
        "                    sampling_steps=sampling_time_step,\n",
        "                    custom_sampling_steps=custom_sampling_steps\n",
        "                )\n",
        "\n",
        "            result.append((inputs, noised_image, denoised_image))\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_-bfyaE9-Zi"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBnmbCR7s054",
        "outputId": "accf4ead-a442-434a-bd8c-f15f1ae97495"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test = MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyrjdgT3d_Yn"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 1000\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 30\n",
        "P_UNCOND = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2iEoXPDpFXh"
      },
      "outputs": [],
      "source": [
        "model = MyDiffusion(\n",
        "    n_timesteps=TIME_STEPS,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    custom_channel_scale=[128, 128, 256, 256, 512, 512],\n",
        "    train_set=train,\n",
        "    test_set=test,\n",
        "    train_batch_size=BATCH_SIZE,\n",
        "    test_batch_size=8,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# MODEL_PATH = '/content/drive/My Drive/models/DDPM_MNIST/UCA128_T1000_E30_v2.pt'\n",
        "# model.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4lkZWaVo24g",
        "outputId": "58fce7cc-374e-4db0-c0ab-df9f88730a10"
      },
      "outputs": [],
      "source": [
        "print(\"model size : \", sum(p.numel() for p in model.g.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLwQ3_CdsgnQ",
        "outputId": "a9a2076c-888f-4cd5-b633-ec238d2a4ad5"
      },
      "outputs": [],
      "source": [
        "history = model.train(\n",
        "    n_epoch=EPOCHS,\n",
        "    p_uncond=P_UNCOND\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "lA7RXKB6ix-z",
        "outputId": "eb9b8b6d-02dd-48a3-c548-f4b5a65f512b"
      },
      "outputs": [],
      "source": [
        "print_seq(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "G09ti-g5dLKc",
        "outputId": "aee460fc-924c-4457-f5c1-2481974e1acd"
      },
      "outputs": [],
      "source": [
        "weights = model.sampling_weights.get_weights()\n",
        "# weights = torch.nn.functional.softmax(weights / 0.1, dim=0)\n",
        "# torch.save(weights, \"weights.pt\")\n",
        "\n",
        "print_seq(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK-c2N5tgKN7"
      },
      "source": [
        "# DDIM Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6H-9Br7H5po"
      },
      "outputs": [],
      "source": [
        "test_noise = torch.randn((1, 1, 32, 32)).to(device)\n",
        "test_noise = test_noise.repeat(10, 1, 1, 1)\n",
        "\n",
        "condition = torch.tensor(list(range(10))).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o1qk17p2eUs"
      },
      "outputs": [],
      "source": [
        "# test_noise = torch.load(\"test_noise.pt\", weights_only=False)\n",
        "# torch.save(test_noise, \"test_noise.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "_q_2fa4NhLEi",
        "outputId": "0fa8114e-cd99-43f4-f496-38e24f87a1a2"
      },
      "outputs": [],
      "source": [
        "test_steps = [1, 2, 3, 4, 5, 10, 20, 50, 100]\n",
        "\n",
        "for steps in test_steps:\n",
        "    test_denoised_image = model.decoder.DDIM_sampling(\n",
        "        test_noise,\n",
        "        TIME_STEPS,\n",
        "        c=condition,\n",
        "        w=1,\n",
        "        sampling_steps=steps\n",
        "    )\n",
        "\n",
        "    print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxSvf1EpFsTL"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKyyCXACk42"
      },
      "source": [
        "### Extract Threshold Timestep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "eIQ2iQzIHwmT",
        "outputId": "2774892a-ad15-4aeb-cc30-60738820da3d"
      },
      "outputs": [],
      "source": [
        "extract_time_step = 100\n",
        "\n",
        "test_denoised_image = model.decoder.DDIM_sampling(\n",
        "    test_noise,\n",
        "    TIME_STEPS,\n",
        "    c=condition,\n",
        "    w=1,\n",
        "    sampling_steps=extract_time_step\n",
        ")\n",
        "\n",
        "print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "ejBk-qibhgA8",
        "outputId": "c502e9bf-d925-4e5a-9cac-05970b3aaf19"
      },
      "outputs": [],
      "source": [
        "diff_norm = reversed(torch.load(\"DDIM_origin.pt\", weights_only=False))\n",
        "FINAL = int(torch.argmax(diff_norm).item() * (1000 / extract_time_step))\n",
        "\n",
        "print(FINAL)\n",
        "print_seq(diff_norm,\n",
        "          title='Difference vs. Timesteps (origin)',\n",
        "          x_label='Timesteps',\n",
        "          y_label='DiffNorm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "aeE9orKlIya7",
        "outputId": "46509d27-ec84-4ae7-b0d6-3215f8b9019e"
      },
      "outputs": [],
      "source": [
        "diff_norm = reversed(torch.load(\"DDIM_prev.pt\", weights_only=False))\n",
        "\n",
        "THRESHOLD = int(torch.argmax(diff_norm).item() * (TIME_STEPS / extract_time_step))\n",
        "print(f\"threshold_timestep : {THRESHOLD}\")\n",
        "\n",
        "print_seq(diff_norm,\n",
        "           title='Difference vs. Timesteps (prev)',\n",
        "           x_label='Timesteps',\n",
        "           y_label='DiffNorm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvLQObse-0aq"
      },
      "source": [
        "### Threshold Sampling Timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqM8cQelFtUN",
        "outputId": "2eafe1d9-054d-4031-e9d7-9f2e16196879"
      },
      "outputs": [],
      "source": [
        "target_step = 4\n",
        "P = target_step // 2\n",
        "S = target_step - P\n",
        "\n",
        "perceptual_steps = list(range(0, THRESHOLD, (THRESHOLD - 0) // P))\n",
        "semantic_steps = list(range(THRESHOLD, TIME_STEPS, (TIME_STEPS - THRESHOLD) // S))\n",
        "total_steps = perceptual_steps + semantic_steps\n",
        "\n",
        "print(total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "K5sFV5M7wqCL",
        "outputId": "57de5ce8-0f90-4c1e-874f-62b77434c657"
      },
      "outputs": [],
      "source": [
        "test_denoised_image = model.decoder.DDIM_sampling(\n",
        "    test_noise,\n",
        "    TIME_STEPS,\n",
        "    c=condition,\n",
        "    w=1,\n",
        "    custom_sampling_steps=total_steps\n",
        ")\n",
        "\n",
        "print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9vudogVBVEm"
      },
      "source": [
        "### Control Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "collapsed": true,
        "id": "jcsDlRibBXSk",
        "outputId": "4ca26f65-3b00-410e-dc47-3f5ffcefb997"
      },
      "outputs": [],
      "source": [
        "for i in range(1, target_step):\n",
        "\n",
        "    perceptual_steps = list(range(0, THRESHOLD, (THRESHOLD - 0) // i))\n",
        "    semantic_steps = list(range(THRESHOLD, TIME_STEPS, (TIME_STEPS - THRESHOLD) // (target_step - i)))\n",
        "    total_steps = perceptual_steps + semantic_steps\n",
        "\n",
        "    print(f\"perceptual steps : {len(perceptual_steps)}, semantic steps : {len(semantic_steps)}\")\n",
        "    print(total_steps)\n",
        "\n",
        "    test_denoised_image = model.decoder.DDIM_sampling(\n",
        "        test_noise,\n",
        "        TIME_STEPS,\n",
        "        c=condition,\n",
        "        w=1,\n",
        "        custom_sampling_steps=total_steps\n",
        "    )\n",
        "\n",
        "    print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHu0L3sTDSco"
      },
      "source": [
        "### Evaluation Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIo55MlvDZf1",
        "outputId": "d7f916c4-a9cc-4191-a8e1-9e700945cace"
      },
      "outputs": [],
      "source": [
        "result = model.evaluate(\n",
        "    epochs = 100,\n",
        "    sampling_type = \"DDIM\",\n",
        "    sampling_time_step = 4,\n",
        "    w = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "B3uuyoiMD9A9",
        "outputId": "b1b5804a-d5ef-423d-b799-6740aea29bb1"
      },
      "outputs": [],
      "source": [
        "print_result(result[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04cSyeJPDqWs",
        "outputId": "e8859323-d8b9-4806-938d-7e9a6f599ad6"
      },
      "outputs": [],
      "source": [
        "print_scores(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCqXxYKNEf19",
        "outputId": "8e170a1e-88f8-4370-ae9a-f6042232183d"
      },
      "outputs": [],
      "source": [
        "result = model.evaluate(\n",
        "    epochs = 100,\n",
        "    sampling_type = \"DDIM\",\n",
        "    custom_sampling_steps = [0, 10, 340, 670],\n",
        "    w = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "r2GsD5c_EqYs",
        "outputId": "f48c6b7d-0a27-42be-cbd2-539b366a9f59"
      },
      "outputs": [],
      "source": [
        "print_result(result[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kjctK_BEsMZ",
        "outputId": "0271c335-9c2f-42b9-dc55-9a84c2bd1648"
      },
      "outputs": [],
      "source": [
        "print_scores(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDQRhQDkVcqv"
      },
      "source": [
        "### Inflection Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADnCO8r4Vcqv"
      },
      "outputs": [],
      "source": [
        "diff_norm = reversed(torch.load(\"DDIM_origin.pt\", weights_only=False))\n",
        "gradient = []\n",
        "for i in range(1, len(diff_norm)):\n",
        "    gradient.append(diff_norm[i] - diff_norm[i - 1])\n",
        "\n",
        "gradient = torch.tensor(gradient)\n",
        "THRESHOLD_INFLECTION = int(torch.argmin(gradient).item() * (TIME_STEPS / extract_time_step))\n",
        "print(THRESHOLD_INFLECTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ3OkWOaVcqv"
      },
      "outputs": [],
      "source": [
        "perceptual_steps = list(range(0, THRESHOLD, (THRESHOLD - 0) // 5))\n",
        "semantic_steps = list(range(THRESHOLD, TIME_STEPS, (TIME_STEPS - THRESHOLD) // 5))\n",
        "total_steps = perceptual_steps + semantic_steps\n",
        "\n",
        "print(total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK0UxZepVcqv"
      },
      "outputs": [],
      "source": [
        "test_denoised_image = model.decoder.DDIM_sampling(\n",
        "    test_noise,\n",
        "    TIME_STEPS,\n",
        "    c=condition,\n",
        "    w=0.5,\n",
        "    custom_sampling_steps=total_steps\n",
        ")\n",
        "\n",
        "print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7BrE41-6ff"
      },
      "source": [
        "### Additional Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnQhOikcxE8C"
      },
      "outputs": [],
      "source": [
        "perceptual_steps = list(range(FINAL, THRESHOLD, (THRESHOLD - FINAL) // 5))\n",
        "semantic_steps = list(range(THRESHOLD, TIME_STEPS, (TIME_STEPS - THRESHOLD) // 5))\n",
        "total_steps = perceptual_steps + semantic_steps\n",
        "\n",
        "print(total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEflNL7j-8iR"
      },
      "outputs": [],
      "source": [
        "test_denoised_image = model.decoder.DDIM_sampling(\n",
        "    test_noise,\n",
        "    TIME_STEPS,\n",
        "    c=condition,\n",
        "    w=1,\n",
        "    custom_sampling_steps=total_steps\n",
        ")\n",
        "\n",
        "print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CG89WAd_Byo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03dcf6e30386447c9d77cbbef20933a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c3b074b11194ad78251c81cfa24c5a6",
              "IPY_MODEL_2b35864dfa5f4f54b05b9ceb56aced02",
              "IPY_MODEL_1a030f3f311c4b84aa5e993b1b15b069"
            ],
            "layout": "IPY_MODEL_59a15d4dd32146099897b79a8220d465"
          }
        },
        "055d44172a0c451d91b90d0e06ecbe2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d1578b5ad884d91ac9fec47e8dfd8af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16b6711d184b4b3c863f3b8c27f92d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a030f3f311c4b84aa5e993b1b15b069": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30e127545a549c98076234fe0464211",
            "placeholder": "​",
            "style": "IPY_MODEL_3f93502fbb7c490094b1767d30286c6c",
            "value": " 985/985 [00:00&lt;00:00, 17.6kB/s]"
          }
        },
        "1abfe2a2083e4befbb208563e1da7b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d1578b5ad884d91ac9fec47e8dfd8af",
            "placeholder": "​",
            "style": "IPY_MODEL_51c59b9a43ef4f2da0028a44f823437a",
            "value": " 343M/343M [00:03&lt;00:00, 130MB/s]"
          }
        },
        "1d65bed6ac0143dc827ea604fe4993fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b35864dfa5f4f54b05b9ceb56aced02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6ddac863e47426884f56676e61e7a08",
            "max": 985,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68c3cc1a43a948549625aeb2fb3f6424",
            "value": 985
          }
        },
        "2f0a888db2884a3c8e356abea37a7444": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c3b074b11194ad78251c81cfa24c5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_055d44172a0c451d91b90d0e06ecbe2d",
            "placeholder": "​",
            "style": "IPY_MODEL_2f0a888db2884a3c8e356abea37a7444",
            "value": "config.json: 100%"
          }
        },
        "3f93502fbb7c490094b1767d30286c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51c59b9a43ef4f2da0028a44f823437a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59a15d4dd32146099897b79a8220d465": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "651e53706ea945fb9d5e914b614c5f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68c3cc1a43a948549625aeb2fb3f6424": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e30336a2a70409586c831ebce165041": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fb6dc0ba48e4b2c8071efd8f9477199": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d65bed6ac0143dc827ea604fe4993fa",
            "max": 228,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_651e53706ea945fb9d5e914b614c5f37",
            "value": 228
          }
        },
        "7fb5c25b7f4a4733807ce4f95026b1f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81bfdb8ca94342e88071c5362b074a75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98896cd4ef184ea28b4b49e7a3d63855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af305e43c47e4855ab0cbeae3e85006b",
            "placeholder": "​",
            "style": "IPY_MODEL_e7820cdb7deb4cd4a6c7cb8abf44aea1",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "9d17e0d0c57e42deaa68a61415e98d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e30336a2a70409586c831ebce165041",
            "max": 343291569,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16b6711d184b4b3c863f3b8c27f92d03",
            "value": 343291569
          }
        },
        "a30e127545a549c98076234fe0464211": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5d90f88c0154f8aa1d7f84b6a30f2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fb5c25b7f4a4733807ce4f95026b1f8",
            "placeholder": "​",
            "style": "IPY_MODEL_e986abb58c3041d5bb0100fd2f240465",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "aa8f2ee6fa2d4d27827af39c3d90de38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98896cd4ef184ea28b4b49e7a3d63855",
              "IPY_MODEL_6fb6dc0ba48e4b2c8071efd8f9477199",
              "IPY_MODEL_c983d1d749cc422fa20d96e0972da286"
            ],
            "layout": "IPY_MODEL_b4abf20e08bd41eba1f1f3c035368de7"
          }
        },
        "af305e43c47e4855ab0cbeae3e85006b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b09170a65040ca850cbf7613174223": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4abf20e08bd41eba1f1f3c035368de7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c983d1d749cc422fa20d96e0972da286": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f720f68d564639bbb3cfd40ce0fb0f",
            "placeholder": "​",
            "style": "IPY_MODEL_b3b09170a65040ca850cbf7613174223",
            "value": " 228/228 [00:00&lt;00:00, 13.1kB/s]"
          }
        },
        "e6ddac863e47426884f56676e61e7a08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7820cdb7deb4cd4a6c7cb8abf44aea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e986abb58c3041d5bb0100fd2f240465": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f122bbefde264cc1af5c4d136862216f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5d90f88c0154f8aa1d7f84b6a30f2fd",
              "IPY_MODEL_9d17e0d0c57e42deaa68a61415e98d57",
              "IPY_MODEL_1abfe2a2083e4befbb208563e1da7b86"
            ],
            "layout": "IPY_MODEL_81bfdb8ca94342e88071c5362b074a75"
          }
        },
        "f7f720f68d564639bbb3cfd40ce0fb0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
