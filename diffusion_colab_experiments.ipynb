{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBCygEM5hLEa",
        "outputId": "f491487a-1ca7-4d71-c645-01842200b452"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -U -q torch torchvision scipy tdqm matplotlib scipy transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeVPd_CFqKVF"
      },
      "source": [
        "# Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayIRMJrwqNR2",
        "outputId": "a7a66bf4-de05-4032-fd5f-4f0203f2dc9a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004v5ZfzxX6u"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikG5UIXCnvh6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "from scipy.linalg import sqrtm\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hptABxxsmq7g"
      },
      "source": [
        "### Print Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJh_Om8QzgEY"
      },
      "outputs": [],
      "source": [
        "def image_normalize(image):\n",
        "    image = image.cpu()\n",
        "    n_channels = image.shape[0]\n",
        "    for channel in range(n_channels):\n",
        "        max_value = torch.max(image[channel])\n",
        "        min_value = torch.min(image[channel])\n",
        "        image[channel] = (image[channel] - min_value) / (max_value - min_value)\n",
        "\n",
        "    image = image.permute(1, 2, 0)\n",
        "\n",
        "    return image\n",
        "\n",
        "def print_image(image):\n",
        "    image = image_normalize(image)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "def print_2images(image1, image2):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(image_normalize(image1))\n",
        "    axes[0].set_title('Image 1')\n",
        "\n",
        "    axes[1].imshow(image_normalize(image2))\n",
        "    axes[1].set_title('Image 2')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_digits(result):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(10, 5))\n",
        "\n",
        "    B = result.shape[0]\n",
        "    for i in range(B):\n",
        "        axes[i].imshow(image_normalize(result[i]))\n",
        "        axes[i].set_title(i)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_result(result):\n",
        "    for original_image, noised_image, denoised_image in result:\n",
        "        batch_size = original_image.shape[0]\n",
        "        for idx in range(batch_size):\n",
        "            print_2images(original_image[idx], denoised_image[idx])\n",
        "            # print_image(image[idx])\n",
        "            # print_image(noised_image[idx])\n",
        "            # print_image(denoised_image[idx])\n",
        "\n",
        "\n",
        "def print_loss(loss_values,\n",
        "               label='Training Loss',\n",
        "               x_label='Epoch',\n",
        "               y_label='Loss',\n",
        "               title='Loss vs. Epochs'):\n",
        "    epochs = list(range(1, len(loss_values) + 1))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs, loss_values, 'b-o', label=label)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0hElUKmq7g"
      },
      "source": [
        "### Torch Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV83NB-7-Ueo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7z-5moymq7h"
      },
      "source": [
        "### Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "c8513cf8f07a44dd8af93b2db38683e0",
            "fb99d71f3c9b4146a302f8c8ed6d2454",
            "9f246808a43c4ec2922951c51c862d37",
            "451b946f1dc646d39eeb85eec70b8c87",
            "a138309d763845b091bed4fee9d8356e",
            "2c437b81dfc24af5be2ca5ac40eb0468",
            "55eb402489a1457e83c051d47d202fc9",
            "e7affe00cdd94a6e98c9b1e235249cc2",
            "c3b50e9ac853462ea4adab3856bcc368",
            "ca1719ce3f3c4ef696bfb515baf9cf8b",
            "694458ad026f48db8a444fceccf0130e",
            "5e1a901b81024c26af435e66ffbad1c1",
            "ca4dfb81261c481c91d7e8b5cbddb451",
            "33cc90106bc64d4798c0af552ddef021",
            "0c4c3e7bbf7646b6bc4be3991d2ec202",
            "97250d9e46d04e1fafce6e4b204e10fa",
            "7c6e740abd2f416fa4849f714928470f",
            "aa03ed662100470f885d59a9233014ff",
            "049faf60564f40929cf10837a85cb182",
            "96fc1566c68d4083bbe5a0617e9c62ac",
            "badb5db711a34d8a9c3a8810411ee0ff",
            "2f4445e401fb4840ba1f92cbc8fdb728",
            "b49f7eb5071a4254bb915f982ea7a6c8",
            "c5e041e3efe54f96b5567536fa98b0af",
            "20c759ad9b1b458abfdf22de18420615",
            "a03f27a94d6641bdb61f1d25ef1d93ca",
            "661c07c474a74f8b8dcb1427b3d2d5ab",
            "01d533e252c64d3dae9376ad6e6fb936",
            "46f15057baa749e78f82df133d09c51e",
            "0385b7c88dc14e82b3e6d76a7f659825",
            "eb655cdd24d8456c831a96bd3d94da54",
            "0ef22d12d8354aadaefaafb86b7b9d43",
            "2ca9142d5076401a84d8f872373102b1"
          ]
        },
        "id": "FLjlFrOZmq7h",
        "outputId": "43fde515-7c36-49ab-a418-cc45b0e90983"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "CLASSIFIER_MODEL = pipeline(\n",
        "    \"image-classification\",\n",
        "    model=\"farleyknight-org-username/vit-base-mnist\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "def inception_ViT(inputs):\n",
        "\n",
        "    def convert_to_pil(x):\n",
        "        converted_images = []\n",
        "        for i in range(x.shape[0]):\n",
        "            converted_images.append(to_pil_image(x[i]))\n",
        "        return converted_images\n",
        "\n",
        "    def convert_classifier_results(results):\n",
        "        prob = [0.00000001] * 10\n",
        "        for result in results:\n",
        "            prob[int(result['label'])] = result['score']\n",
        "        return prob\n",
        "\n",
        "    # inputs : [B, 1, 32, 32]\n",
        "    out = CLASSIFIER_MODEL(convert_to_pil(inputs))\n",
        "    out = [convert_classifier_results(x) for x in out]\n",
        "    return out\n",
        "\n",
        "# Function to get inception features\n",
        "def get_inception_features(inception_model, result):\n",
        "    target, origin = [], []\n",
        "\n",
        "    for original_image, noised_image, denoised_image in result:\n",
        "        # denoised_image : [B, 1, 32, 32]\n",
        "        origin += inception_model(original_image)\n",
        "        target += inception_model(denoised_image)\n",
        "\n",
        "    return origin, target\n",
        "\n",
        "# Calculate FID\n",
        "def calculate_fid(origin, target):\n",
        "    mu1, sigma1 = np.mean(origin, axis=0), np.cov(origin, rowvar=False)\n",
        "    mu2, sigma2 = np.mean(target, axis=0), np.cov(target, rowvar=False)\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "def calculate_inception_score(results):\n",
        "    scores = []\n",
        "    for part in results:\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores.append(np.exp(np.mean([np.sum(p * np.log(p / py)) for p in part])))\n",
        "    return np.mean(scores), np.std(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWXFvNsVmq7h"
      },
      "outputs": [],
      "source": [
        "def print_scores(result):\n",
        "    origin, target = get_inception_features(inception_ViT, result)\n",
        "\n",
        "    origin_IS_mean, origin_IS_std = calculate_inception_score(origin)\n",
        "    print(f'[Origin] IS: {origin_IS_mean} ± {origin_IS_std}')\n",
        "\n",
        "    target_IS_mean, target_IS_std = calculate_inception_score(target)\n",
        "    print(f'[Target] IS: {target_IS_mean} ± {target_IS_std}')\n",
        "\n",
        "    FID = calculate_fid(origin, target)\n",
        "    print(f'FID: {FID}')\n",
        "\n",
        "    return origin_IS_mean, target_IS_mean, FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHS6FSDSn2hy"
      },
      "source": [
        "# Noise Scheduler\n",
        "- betas, alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq0_qYUenyDg"
      },
      "outputs": [],
      "source": [
        "class NoiseSchedule:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps,\n",
        "        beta_start = 0.0001,\n",
        "        beta_end = 0.02,\n",
        "        init_type = \"linear\",\n",
        "        device = device\n",
        "    ) -> None:\n",
        "\n",
        "        self._size = n_timesteps\n",
        "        if init_type == \"linear\":\n",
        "            self._betas = torch.linspace(beta_start, beta_end, n_timesteps).to(device)\n",
        "        if init_type == \"exponential\":\n",
        "            self._betas = torch.from_numpy(np.geomspace(beta_start, beta_end, n_timesteps)).to(device)\n",
        "        self._alphas = self._calculate_alphas()\n",
        "\n",
        "    def _calculate_alphas(self):\n",
        "        self._alphas = torch.cumprod(1 - self._betas, axis=0)\n",
        "        return self._alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8PxHOV6n-ZV"
      },
      "source": [
        "# Forward Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHr0pKLWn5tI"
      },
      "outputs": [],
      "source": [
        "class ForwardEncoder:\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        noise_schedule,\n",
        "        device = None\n",
        "    ):\n",
        "        self.noise_schedule = noise_schedule\n",
        "        self.device = device\n",
        "\n",
        "    def noise(self, data, time_step):\n",
        "        # time_step : [B]\n",
        "        # data : [B, 1, 32, 32]\n",
        "\n",
        "        alpha = self.noise_schedule._alphas[time_step]\n",
        "        alpha = alpha.reshape(-1, 1, 1, 1)\n",
        "        # alpha : [B, 1, 1, 1]\n",
        "\n",
        "        epsilon = torch.randn(data.shape).to(self.device)\n",
        "        # torch.randn ~ N(0, 1)\n",
        "\n",
        "        return torch.sqrt(alpha) * data + torch.sqrt(1 - alpha) * epsilon, epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BIC7rsQ8en-"
      },
      "source": [
        "# Reverse Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV7lE5EZzyuy"
      },
      "outputs": [],
      "source": [
        "class ReverseDecoder:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        noise_schedule,\n",
        "        g,\n",
        "        device = None\n",
        "    ):\n",
        "        self.noise_schedule = noise_schedule\n",
        "        self.device = device\n",
        "        self.g = g\n",
        "\n",
        "    def DDPM_sampling(\n",
        "        self,\n",
        "        noise_data,\n",
        "        time_step,\n",
        "        c = None,\n",
        "        w = 0\n",
        "    ):\n",
        "        # noise_data : [B, 1, 32, 32]\n",
        "        # c : [B]\n",
        "        # time_step : INT\n",
        "\n",
        "        origin_data = noise_data.clone()\n",
        "        batch_size = noise_data.shape[0]\n",
        "        # batch_size : B\n",
        "\n",
        "        history_with_origin = []\n",
        "        history_with_prev = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # step : [T - 1, T - 2, .. 2, 1, 0]\n",
        "            for step in range(time_step - 1, -1, -1):\n",
        "\n",
        "                t = torch.full((batch_size, ), step).to(self.device)\n",
        "                t = t.reshape(-1, 1, 1, 1)\n",
        "                # t : [B, 1, 1, 1]\n",
        "\n",
        "                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "                mu = 1 / torch.sqrt(1 - self.noise_schedule._betas[t]) * (noise_data - (self.noise_schedule._betas[t] / (1 - self.noise_schedule._alphas[t])) * predict_noise)\n",
        "                # mu : [B, 1, 32, 32]\n",
        "\n",
        "                if step == 0:\n",
        "                    # if t == 0, no add noise\n",
        "                    break\n",
        "\n",
        "                epsilon = torch.randn(noise_data.shape).to(self.device)\n",
        "                new_data = mu + torch.sqrt(self.noise_schedule._betas[t]) * epsilon\n",
        "\n",
        "                history_with_origin.append(torch.norm(origin_data - noise_data))\n",
        "                history_with_prev.append(torch.norm(new_data - noise_data))\n",
        "                noise_data = new_data\n",
        "\n",
        "        torch.save(torch.tensor(history_with_origin), \"DDPM_origin.pt\")\n",
        "        torch.save(torch.tensor(history_with_prev), \"DDPM_prev.pt\")\n",
        "\n",
        "        return noise_data\n",
        "\n",
        "    def DDIM_sampling(\n",
        "        self,\n",
        "        noise_data,\n",
        "        time_step,\n",
        "        c = None,\n",
        "        w = 0,\n",
        "        sampling_steps = 10,\n",
        "        sampling_types = \"linear\",\n",
        "        custom_sampling_steps = None\n",
        "    ):\n",
        "        # noise_data : [B, 1, 32, 32]\n",
        "        # c : [B]\n",
        "        # time_step : INT\n",
        "\n",
        "        B = noise_data.shape[0]\n",
        "        tau = None\n",
        "        \n",
        "        if sampling_types == \"linear\":\n",
        "            tau = list(range(0, time_step, time_step // sampling_steps))\n",
        "        if sampling_types == \"exponential\":\n",
        "            tau = list(np.geomspace(1, time_step, time_step // sampling_steps))\n",
        "            \n",
        "        if custom_sampling_steps is not None:\n",
        "            tau = custom_sampling_steps\n",
        "\n",
        "        S = len(tau)\n",
        "\n",
        "        origin_data = noise_data.clone()\n",
        "        history_with_origin = []\n",
        "        history_with_prev = []\n",
        "\n",
        "        # batch_size : B\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # step : [T - 1, T - 2, .. 2, 1, 0]\n",
        "            for i in range(S - 1, -1, -1):\n",
        "\n",
        "                t = torch.full((B, ), tau[i]).to(self.device)\n",
        "                t = t.reshape(-1, 1, 1, 1)\n",
        "                alpha_t = self.noise_schedule._alphas[t]\n",
        "\n",
        "                alpha_t_1 = torch.full((B, 1, 1, 1,), 1).to(self.device)\n",
        "                if i - 1 >= 0:\n",
        "                    t_1 = torch.full((B, ), tau[i - 1]).to(self.device)\n",
        "                    t_1 = t_1.reshape(-1, 1, 1, 1)\n",
        "                    alpha_t_1 = self.noise_schedule._alphas[t_1]\n",
        "\n",
        "                predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "                new_data = torch.sqrt(alpha_t_1) * ((noise_data - torch.sqrt(1 - alpha_t) * predict_noise) / torch.sqrt(alpha_t)) + torch.sqrt(1 - alpha_t_1) * predict_noise\n",
        "\n",
        "                history_with_origin.append(torch.norm(origin_data - noise_data))\n",
        "                history_with_prev.append(torch.norm(new_data - noise_data))\n",
        "                noise_data = new_data\n",
        "\n",
        "        torch.save(torch.tensor(history_with_origin), \"DDIM_origin.pt\")\n",
        "        torch.save(torch.tensor(history_with_prev), \"DDIM_prev.pt\")\n",
        "        return noise_data\n",
        "\n",
        "\n",
        "    def DDIM_sampling_step(\n",
        "        self,\n",
        "        noise_data,\n",
        "        t,\n",
        "        c = None,\n",
        "        w = 1,\n",
        "        t_1 = None\n",
        "    ):\n",
        "\n",
        "        t = t.reshape(-1, 1, 1, 1)\n",
        "        if t_1 is None:\n",
        "            t_1 = torch.clamp(t - 1, min=0)\n",
        "\n",
        "        alpha_t = self.noise_schedule._alphas[t]\n",
        "        alpha_t_1 = self.noise_schedule._alphas[t_1]\n",
        "\n",
        "        predict_noise = (1 + w) * self.g(noise_data, t, c) - w * self.g(noise_data, t)\n",
        "        V1 = torch.sqrt(alpha_t_1) * ((noise_data - torch.sqrt(1 - alpha_t) * predict_noise) / torch.sqrt(alpha_t))\n",
        "        V2 = torch.sqrt(1 - alpha_t_1) * predict_noise\n",
        "\n",
        "        return V1 + V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0pbKzSh82Og"
      },
      "source": [
        "# UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG_ujnAqOf-H"
      },
      "source": [
        "### Backbones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSR1El6-Of-I"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, num_steps, time_emb_dim) -> None:\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        self.time_embed = nn.Embedding(num_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = self.sinusoidal_embedding(num_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "\n",
        "    def sinusoidal_embedding(self, n, d):\n",
        "        # Returns the standard positional embedding\n",
        "        embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n",
        "        sin_mask = torch.arange(0, n, 2)\n",
        "        embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
        "        embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.time_embed(input)\n",
        "\n",
        "\n",
        "class WideResNetBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        n = 3,\n",
        "        kernel_size = 3,\n",
        "        stride = 1,\n",
        "        padding = 1,\n",
        "        num_groups = 32\n",
        "    ):\n",
        "        super(WideResNetBlock, self).__init__()\n",
        "        self.n = n\n",
        "        self.ks = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if kernel_size != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n",
        "                # nn.BatchNorm2d(out_size)\n",
        "            )\n",
        "\n",
        "        if is_batchnorm:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                    #  nn.BatchNorm2d(out_size),\n",
        "                    nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n",
        "                    nn.SiLU(inplace=True)\n",
        "                )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_channels = out_channels\n",
        "\n",
        "        else:\n",
        "            for i in range(1, n + 1):\n",
        "                conv = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "                    nn.SiLU(inplace=True)\n",
        "                )\n",
        "                setattr(self, 'conv%d' % i, conv)\n",
        "                in_channels = out_channels\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        for i in range(1, self.n + 1):\n",
        "            conv = getattr(self, 'conv%d' % i)\n",
        "            x = conv(x)\n",
        "        x += self.shortcut(inputs)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        num_heads = 2,\n",
        "        num_groups = 32,\n",
        "    ):\n",
        "        super(MultiHeadAttentionBlock, self).__init__()\n",
        "\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "        # For each of heads use d_k = d_v = d_model / num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = out_channels\n",
        "        self.d_keys = out_channels // num_heads\n",
        "        self.d_values = out_channels // num_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.W_K = nn.Linear(in_channels, out_channels, bias=False)\n",
        "        self.W_V = nn.Linear(in_channels, out_channels, bias=False)\n",
        "\n",
        "        self.final_projection = nn.Linear(out_channels, out_channels, bias=False)\n",
        "        self.norm = nn.GroupNorm(num_channels=out_channels, num_groups=num_groups)\n",
        "\n",
        "    def split_features_for_heads(self, tensor):\n",
        "        batch, hw, emb_dim = tensor.shape\n",
        "        channels_per_head = emb_dim // self.num_heads\n",
        "        heads_splitted_tensor = torch.split(tensor, split_size_or_sections=channels_per_head, dim=-1)\n",
        "        heads_splitted_tensor = torch.stack(heads_splitted_tensor, 1)\n",
        "        return heads_splitted_tensor\n",
        "\n",
        "    def attention(self, q, k, v):\n",
        "\n",
        "        B, C, H, W = q.shape\n",
        "        q = q.view(B, C, q.shape[2] * q.shape[3]).transpose(1, 2)\n",
        "        k = k.view(B, C, k.shape[2] * k.shape[3]).transpose(1, 2)\n",
        "        v = v.view(B, C, v.shape[2] * v.shape[3]).transpose(1, 2)\n",
        "\n",
        "        # [B, H * W, C_in]\n",
        "\n",
        "        q = self.W_Q(q)\n",
        "        k = self.W_K(k)\n",
        "        v = self.W_V(v)\n",
        "        # N = H * W\n",
        "        # [B, N, C_out]\n",
        "\n",
        "        Q = self.split_features_for_heads(q)\n",
        "        K = self.split_features_for_heads(k)\n",
        "        V = self.split_features_for_heads(v)\n",
        "        # [B, num_heads, N, C_out / num_heads]\n",
        "\n",
        "        scale = self.d_keys ** -0.5\n",
        "        attention_scores = torch.softmax(torch.matmul(Q, K.transpose(-1, -2)) * scale, dim=-1)\n",
        "        attention_scores = torch.matmul(attention_scores, V)\n",
        "        # [B, num_heads, N, C_out / num_heads]\n",
        "\n",
        "        attention_scores = attention_scores.permute(0, 2, 1, 3).contiguous()\n",
        "        # [B, num_heads, N, C_out / num_heads] --> [B, N, num_heads, C_out / num_heads]\n",
        "\n",
        "        concatenated_heads_attention_scores = attention_scores.view(B, H * W, self.d_model)\n",
        "        # [B, N, num_heads, C_out / num_heads] --> [batch, N, C_out]\n",
        "\n",
        "        linear_projection = self.final_projection(concatenated_heads_attention_scores)\n",
        "        linear_projection = linear_projection.transpose(-1, -2).reshape(B, self.d_model, H, W)\n",
        "        # [B, N, C_out] -> [B, C_out, N] -> [B, C_out, H, W]\n",
        "\n",
        "        # Residual connection + norm\n",
        "        out = linear_projection\n",
        "        if self.is_batchnorm:\n",
        "            v = v.transpose(-1, -2).reshape(B, self.d_model, H, W)\n",
        "            out = self.norm(out + v)\n",
        "        return out\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        return self.attention(q, k, v)\n",
        "\n",
        "\n",
        "class SelfAttentionBlock(MultiHeadAttentionBlock):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        is_batchnorm = True,\n",
        "        num_heads = 2,\n",
        "        num_groups = 32,\n",
        "    ):\n",
        "        super().__init__(in_channels, out_channels, num_heads, num_groups)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x, x, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eJz64ADOf-I"
      },
      "source": [
        "### UNet Body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olBnE6_v81Nf"
      },
      "outputs": [],
      "source": [
        "class UNetDown(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        base_model = WideResNetBlock,\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNetDown, self).__init__()\n",
        "        self.conv = base_model(in_channels, out_channels, is_batchnorm=is_batchnorm)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        base_model = WideResNetBlock,\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNetUp, self).__init__()\n",
        "        self.conv = base_model(out_channels * 2, out_channels, is_batchnorm=is_batchnorm)\n",
        "        if is_deconv:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "    def forward(self, inputs0, *input):\n",
        "        outputs0 = self.up(inputs0)\n",
        "        for i in range(len(input)):\n",
        "            outputs0 = torch.cat([outputs0, input[i]], 1)\n",
        "        return self.conv(outputs0)\n",
        "\n",
        "\n",
        "class UNetTimeEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_in, dim_out) -> None:\n",
        "        super(UNetTimeEmbedding, self).__init__()\n",
        "        self.ln = nn.Linear(dim_in, dim_out)\n",
        "        self.activation = nn.SiLU()\n",
        "        self.ln2 = nn.Linear(dim_out, dim_out)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B = inputs.shape[0]\n",
        "\n",
        "        x = self.ln(inputs)\n",
        "        x = self.activation(x)\n",
        "        x = self.ln2(x)\n",
        "\n",
        "        return x.reshape(B, -1, 1, 1)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels = 1,\n",
        "        out_channels = 1,\n",
        "        n_steps = 1000,\n",
        "        time_emb_dim = 256,\n",
        "        n_classes = 10,\n",
        "        class_emb_dim = 64,\n",
        "        channel_scale = 64,\n",
        "        num_channel_scale = 5,\n",
        "        custom_channel_scale = None,\n",
        "        cross_attention_layer_indices = [-1],\n",
        "        self_attention_layer_indices = [-1],\n",
        "        is_deconv = True,\n",
        "        is_batchnorm = True\n",
        "    ):\n",
        "        super(UNet, self).__init__()\n",
        "        self.is_deconv = is_deconv\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.is_batchnorm = is_batchnorm\n",
        "\n",
        "        self.self_attention_layer_indices = set(self_attention_layer_indices)\n",
        "        self.cross_attention_layer_indices = set(cross_attention_layer_indices)\n",
        "\n",
        "        # time embedding\n",
        "        self.time_embedding = PositionalEmbedding(n_steps, time_emb_dim)\n",
        "\n",
        "        # conditional variable embedding\n",
        "        self.context_embedding = PositionalEmbedding(n_classes, class_emb_dim)\n",
        "\n",
        "        if custom_channel_scale is None:\n",
        "            # channel exponenetial scales with `channel_scale`\n",
        "            # 64, 128, 256, 512, 1024\n",
        "            filters = [channel_scale * (2 ** i) for i in range(num_channel_scale)]\n",
        "        else:\n",
        "            # custom channel scales\n",
        "            num_channel_scale = len(custom_channel_scale)\n",
        "            filters = custom_channel_scale\n",
        "\n",
        "        self.num_layers = num_channel_scale\n",
        "\n",
        "        # Downsampling\n",
        "        filters.insert(0, in_channels)\n",
        "\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "            base_model = WideResNetBlock\n",
        "            if (layer_idx - self.num_layers) in self.self_attention_layer_indices:\n",
        "                base_model = SelfAttentionBlock\n",
        "\n",
        "            conv = UNetDown(in_channels=filters[layer_idx - 1],\n",
        "                            out_channels=filters[layer_idx],\n",
        "                            is_batchnorm=self.is_batchnorm,\n",
        "                            base_model=base_model)\n",
        "            temb = UNetTimeEmbedding(time_emb_dim, filters[layer_idx])\n",
        "            cemb = UNetTimeEmbedding(class_emb_dim, filters[layer_idx])\n",
        "            maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "            if (layer_idx - self.num_layers) in self.cross_attention_layer_indices:\n",
        "                cross_attention = MultiHeadAttentionBlock(\n",
        "                    in_channels=filters[layer_idx],\n",
        "                    out_channels=filters[layer_idx],\n",
        "                    is_batchnorm=False\n",
        "                )\n",
        "                setattr(self, 'down_cross_attention%d' % layer_idx, cross_attention)\n",
        "\n",
        "            setattr(self, 'down_conv%d' % layer_idx, conv)\n",
        "            setattr(self, 'down_temb%d' % layer_idx, temb)\n",
        "            setattr(self, 'down_cemb%d' % layer_idx, cemb)\n",
        "            setattr(self, 'down_maxpool%d' % layer_idx, maxpool)\n",
        "\n",
        "\n",
        "        # Bottleneck\n",
        "\n",
        "        self.center = UNetDown(filters[-2], filters[-1], is_batchnorm=self.is_batchnorm)\n",
        "        self.temb_center = UNetTimeEmbedding(time_emb_dim, filters[-1])\n",
        "        self.cemb_center = UNetTimeEmbedding(class_emb_dim, filters[-1])\n",
        "        self.cross_attention_center = MultiHeadAttentionBlock(\n",
        "            in_channels=filters[-1],\n",
        "            out_channels=filters[-1],\n",
        "            is_batchnorm=False\n",
        "        )\n",
        "\n",
        "        # upsampling\n",
        "        filters[0] = out_channels\n",
        "\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "            base_model = WideResNetBlock\n",
        "            if (layer_idx - self.num_layers) in self.self_attention_layer_indices:\n",
        "                base_model = SelfAttentionBlock\n",
        "            conv = UNetUp(filters[layer_idx + 1],\n",
        "                          filters[layer_idx],\n",
        "                          is_deconv=self.is_deconv,\n",
        "                          is_batchnorm=self.is_batchnorm,\n",
        "                          base_model=base_model)\n",
        "            temb = UNetTimeEmbedding(time_emb_dim, filters[layer_idx])\n",
        "\n",
        "            setattr(self, 'up_conv%d' % layer_idx, conv)\n",
        "            setattr(self, 'up_temb%d' % layer_idx, temb)\n",
        "\n",
        "        # output\n",
        "        self.outconv = nn.Conv2d(filters[1], self.out_channels, 3, padding=1)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        inputs,\n",
        "        t,\n",
        "        c = None\n",
        "    ):\n",
        "\n",
        "        t = self.time_embedding(t)\n",
        "        if c is not None:\n",
        "            c = self.context_embedding(c)\n",
        "\n",
        "        # inputs : [B, 1, 32, 32]\n",
        "\n",
        "        x = inputs\n",
        "        downsampling_result = [None]\n",
        "\n",
        "        # DOWN-SAMPLING\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "\n",
        "            conv = getattr(self, 'down_conv%d' % layer_idx)\n",
        "            temb = getattr(self, 'down_temb%d' % layer_idx)\n",
        "            cemb = getattr(self, 'down_cemb%d' % layer_idx)\n",
        "            maxpool = getattr(self, 'down_maxpool%d' % layer_idx)\n",
        "\n",
        "            x = conv(x)\n",
        "            downsampling_result.append(x)\n",
        "\n",
        "            if c is not None and (layer_idx - self.num_layers) in self.cross_attention_layer_indices:\n",
        "                CA = getattr(self, 'down_cross_attention%d' % layer_idx)\n",
        "                context_emb = cemb(c)\n",
        "                x = CA(x, context_emb, context_emb)\n",
        "\n",
        "            x += temb(t)\n",
        "            x = maxpool(x)\n",
        "\n",
        "        # BOTTLENECK\n",
        "\n",
        "        x = self.center(x)\n",
        "        if c is not None:\n",
        "            context_emb = self.cemb_center(c)\n",
        "            x = self.cross_attention_center(x, context_emb, context_emb)\n",
        "        x += self.temb_center(t)\n",
        "\n",
        "        # UP-SAMPLING\n",
        "\n",
        "        for layer_idx in range(self.num_layers - 1, 0, -1):\n",
        "            conv = getattr(self, 'up_conv%d' % layer_idx)\n",
        "            temb = getattr(self, 'up_temb%d' % layer_idx)\n",
        "            x = conv(x, downsampling_result[layer_idx])\n",
        "            x += temb(t)\n",
        "\n",
        "        return self.outconv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdKlF90S9eIC"
      },
      "source": [
        "# Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn5dfm7k9eiG"
      },
      "outputs": [],
      "source": [
        "class MyDiffusion:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_timesteps,\n",
        "        train_set = None,\n",
        "        test_set = None,\n",
        "        in_channels = 1,\n",
        "        out_channels = 1,\n",
        "        channel_scale = 64,\n",
        "        num_channle_scale = 5,\n",
        "        train_batch_size = 8,\n",
        "        test_batch_size = 8,\n",
        "        custom_channel_scale = None,\n",
        "        learning_rate = 0.0001,\n",
        "        device = None\n",
        "    ):\n",
        "\n",
        "        self.n_timesteps = n_timesteps\n",
        "        self.channel_scale = channel_scale\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # UNet for predicting total noise\n",
        "        self.g = UNet(in_channels=in_channels,\n",
        "                      out_channels=out_channels,\n",
        "                      n_steps=n_timesteps,\n",
        "                      channel_scale=channel_scale,\n",
        "                      num_channel_scale=num_channle_scale,\n",
        "                      custom_channel_scale=custom_channel_scale)\n",
        "        self.g = self.g.to(device)\n",
        "\n",
        "        # alpha, betas\n",
        "        self.noise_schedule = NoiseSchedule(\n",
        "            n_timesteps=n_timesteps,\n",
        "            device=device\n",
        "            # init_type=\"exponential\",\n",
        "        )\n",
        "\n",
        "        # forward encoder\n",
        "        self.encoder = ForwardEncoder(\n",
        "            noise_schedule=self.noise_schedule,\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        self.decoder = ReverseDecoder(\n",
        "            noise_schedule=self.noise_schedule, \n",
        "            g=self.g,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # optimizer\n",
        "        self.lossFunction = torch.nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.g.parameters(), lr=learning_rate)\n",
        "\n",
        "        # datasets\n",
        "        if train_set:\n",
        "            self.training_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True)\n",
        "        if test_set:\n",
        "            self.testing_loader = DataLoader(test_set, batch_size=test_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    def save(self, path='./model.pt'):\n",
        "        torch.save(self.g.state_dict(), path)\n",
        "\n",
        "\n",
        "    def load(self, path='./model.pt'):\n",
        "        self.g.load_state_dict(torch.load(path))\n",
        "        self.g.eval()\n",
        "\n",
        "\n",
        "    def train_one_epoch(\n",
        "        self,\n",
        "        n_iter_limit = None,\n",
        "        p_uncond = 0.1\n",
        "    ):\n",
        "\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, data in enumerate(tqdm(self.training_loader)):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs, label = data\n",
        "            inputs = inputs.to(self.device)\n",
        "            # print(inputs.shape)\n",
        "\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            # sampled timestep and conditional variables\n",
        "            t = torch.randint(0, self.n_timesteps, (batch_size, )).to(self.device)\n",
        "            c = label.to(self.device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image, epsilon = self.encoder.noise(inputs, t)\n",
        "\n",
        "            outputs = None\n",
        "            if torch.rand((1, )).item() < p_uncond:\n",
        "                outputs = self.g(noised_image, t)\n",
        "            else:\n",
        "                outputs = self.g(noised_image, t, c)\n",
        "\n",
        "            loss = self.lossFunction(outputs, epsilon)\n",
        "\n",
        "            # Adjust learning weights\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i == n_iter_limit:\n",
        "                break\n",
        "\n",
        "        return running_loss / len(self.training_loader)\n",
        "\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        n_epoch = 5,\n",
        "        n_iter_limit = None,\n",
        "        p_uncond = 0.1\n",
        "    ):\n",
        "\n",
        "        history = []\n",
        "\n",
        "        for epoch in range(n_epoch):\n",
        "            print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "            # Make sure gradient tracking is on, and do a pass over the data\n",
        "            self.g.train(True)\n",
        "            avg_loss = self.train_one_epoch(n_iter_limit=n_iter_limit,\n",
        "                                            p_uncond=p_uncond)\n",
        "            history.append(avg_loss)\n",
        "            print('# epoch {} avg_loss: {}'.format(epoch + 1, avg_loss))\n",
        "\n",
        "            model_path = 'U{}_T{}_E{}.pt'.format(self.channel_scale,\n",
        "                                                             self.n_timesteps,\n",
        "                                                             epoch + 1)\n",
        "            torch.save(self.g.state_dict(), model_path)\n",
        "            torch.save(torch.tensor(history), 'history.pt')\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "    def evaluate(\n",
        "        self,\n",
        "        epochs = None,\n",
        "        sampling_type = \"DDPM\",\n",
        "        sampling_time_step = 10,\n",
        "        custom_sampling_steps = None,\n",
        "        w = 0\n",
        "    ):\n",
        "        self.decoder.g = self.g\n",
        "        result = []\n",
        "        for i, data in enumerate(tqdm(self.testing_loader)):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs, label = data # data['image']\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            batch_size = inputs.shape[0]\n",
        "\n",
        "            # timestep\n",
        "            t = torch.full((batch_size, ), self.n_timesteps - 1).to(self.device)\n",
        "            c = label.to(self.device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image, epsilon = self.encoder.noise(inputs, t)\n",
        "\n",
        "            # denoised image\n",
        "            denoised_image = None\n",
        "            if sampling_type == \"DDPM\":\n",
        "                denoised_image = self.decoder.DDPM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w\n",
        "                )\n",
        "            if sampling_type == \"DDIM\":\n",
        "                denoised_image = self.decoder.DDIM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w,\n",
        "                    sampling_steps=sampling_time_step,\n",
        "                    custom_sampling_steps=custom_sampling_steps\n",
        "                )\n",
        "\n",
        "            result.append((inputs, noised_image, denoised_image))\n",
        "\n",
        "            if i == epochs - 1:\n",
        "                break\n",
        "\n",
        "        return result\n",
        "\n",
        "    def evaluate_with_noise(\n",
        "        self,\n",
        "        epochs = 10,\n",
        "        sampling_type = \"DDPM\",\n",
        "        sampling_time_step = 10,\n",
        "        custom_sampling_steps = None,\n",
        "        w = 0\n",
        "    ):\n",
        "        self.decoder.g = self.g\n",
        "\n",
        "        B = self.test_batch_size\n",
        "        result = []\n",
        "        for i in range(epochs):\n",
        "\n",
        "            # inputs = [B, 1, 32, 32]\n",
        "            inputs = torch.randn((B, 1, 32, 32)).to(self.device)\n",
        "\n",
        "            # timestep and context\n",
        "            t = torch.full((B, ), self.n_timesteps - 1).to(self.device)\n",
        "            c = torch.randint(0, 10, (B, )).to(self.device)\n",
        "\n",
        "            # outputs = [B, 1, 28, 28]\n",
        "            noised_image = inputs\n",
        "\n",
        "            # denoised image\n",
        "            denoised_image = None\n",
        "            if sampling_type == \"DDPM\":\n",
        "                denoised_image = self.decoder.DDPM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w\n",
        "                )\n",
        "            if sampling_type == \"DDIM\":\n",
        "                denoised_image = self.decoder.DDIM_sampling(\n",
        "                    noised_image,\n",
        "                    self.n_timesteps,\n",
        "                    c=c,\n",
        "                    w=w,\n",
        "                    sampling_steps=sampling_time_step,\n",
        "                    custom_sampling_steps=custom_sampling_steps\n",
        "                )\n",
        "\n",
        "            result.append((inputs, noised_image, denoised_image))\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_-bfyaE9-Zi"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBnmbCR7s054",
        "outputId": "1a825b9b-c142-44a9-b82e-dbb01e2a6562"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test = MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyrjdgT3d_Yn"
      },
      "outputs": [],
      "source": [
        "TIME_STEPS = 1000\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 30\n",
        "P_UNCOND = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2iEoXPDpFXh",
        "outputId": "f03c8c51-5743-44b0-9284-eec34047ed64"
      },
      "outputs": [],
      "source": [
        "model = MyDiffusion(\n",
        "    n_timesteps=TIME_STEPS,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    custom_channel_scale=[128, 128, 256, 256, 512, 512],\n",
        "    train_set=train,\n",
        "    test_set=test,\n",
        "    train_batch_size=BATCH_SIZE,\n",
        "    test_batch_size=8,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "MODEL_PATH = '/content/drive/My Drive/models/DDPM_MNIST/UCA128_T1000_E30_v2.pt'\n",
        "model.load(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4lkZWaVo24g",
        "outputId": "55f236bb-3e86-405c-d15f-fc7a2d858247"
      },
      "outputs": [],
      "source": [
        "print(\"model size : \", sum(p.numel() for p in model.g.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLwQ3_CdsgnQ"
      },
      "outputs": [],
      "source": [
        "# history = model.train(\n",
        "#     n_epoch=EPOCHS,\n",
        "#     p_uncond=P_UNCOND\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK-c2N5tgKN7"
      },
      "source": [
        "# DDIM Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6H-9Br7H5po"
      },
      "outputs": [],
      "source": [
        "test_noise = torch.randn((1, 1, 32, 32)).to(device)\n",
        "test_noise = test_noise.repeat(10, 1, 1, 1)\n",
        "\n",
        "condition = torch.tensor(list(range(10))).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o1qk17p2eUs"
      },
      "outputs": [],
      "source": [
        "test_noise = torch.load(\"test_noise.pt\", weights_only=False)\n",
        "# torch.save(test_noise, \"test_noise.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "_q_2fa4NhLEi",
        "outputId": "30549061-fc0b-4e18-e07f-3c8bdae9dc71"
      },
      "outputs": [],
      "source": [
        "test_steps = [5, 10, 20, 50, 100]\n",
        "\n",
        "for steps in test_steps:\n",
        "    test_denoised_image = model.decoder.DDIM_sampling(\n",
        "        test_noise,\n",
        "        TIME_STEPS,\n",
        "        c=condition,\n",
        "        w=1,\n",
        "        sampling_steps=steps\n",
        "    )\n",
        "\n",
        "    print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxSvf1EpFsTL"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKyyCXACk42"
      },
      "source": [
        "### Extract Threshold Timestep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "eIQ2iQzIHwmT",
        "outputId": "9d82dc55-d3de-45e8-efb4-6daa187ca33e"
      },
      "outputs": [],
      "source": [
        "extract_time_step = 100\n",
        "\n",
        "test_denoised_image = model.decoder.DDIM_sampling(\n",
        "    test_noise,\n",
        "    TIME_STEPS,\n",
        "    # c=condition,\n",
        "    w=1,\n",
        "    sampling_steps=extract_time_step\n",
        ")\n",
        "\n",
        "print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "ejBk-qibhgA8",
        "outputId": "b75a1d7a-9bb9-4ab5-c018-e17ce322ad2d"
      },
      "outputs": [],
      "source": [
        "diff_norm = reversed(torch.load(\"diff_norm_origin.pt\", weights_only=False))\n",
        "final_timestep = int(torch.argmax(diff_norm).item() * (1000 / extract_time_step))\n",
        "\n",
        "print(final_timestep)\n",
        "print_loss(diff_norm,\n",
        "           title='Difference vs. Timesteps (origin)',\n",
        "           x_label='Timesteps',\n",
        "           y_label='DiffNorm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "aeE9orKlIya7",
        "outputId": "bea4bf20-be86-428c-e902-28b7f24f425b"
      },
      "outputs": [],
      "source": [
        "diff_norm = reversed(torch.load(\"diff_norm_prev.pt\", weights_only=False))\n",
        "threshold_timestep = int(torch.argmax(diff_norm).item() * (1000 / extract_time_step))\n",
        "print(f\"threshold_timestep : {threshold_timestep}\")\n",
        "\n",
        "print_loss(diff_norm,\n",
        "           title='Difference vs. Timesteps (prev)',\n",
        "           x_label='Timesteps',\n",
        "           y_label='DiffNorm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvLQObse-0aq"
      },
      "source": [
        "### Threshold Sampling Timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqM8cQelFtUN",
        "outputId": "2308daa8-b03f-47c6-8b04-9540cce01087"
      },
      "outputs": [],
      "source": [
        "perceptual_steps = list(range(0, threshold_timestep, (threshold_timestep - 0) // 5))\n",
        "semantic_steps = list(range(threshold_timestep, TIME_STEPS, (TIME_STEPS - threshold_timestep) // 5))\n",
        "total_steps = perceptual_steps + semantic_steps\n",
        "\n",
        "print(total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "K5sFV5M7wqCL",
        "outputId": "eb8b7235-d44c-482a-b451-a3ac6c52dbd3"
      },
      "outputs": [],
      "source": [
        "test_denoised_image = model.decoder.DDIM_sampling(\n",
        "    test_noise,\n",
        "    TIME_STEPS,\n",
        "    # c=condition,\n",
        "    w=1,\n",
        "    custom_sampling_steps=total_steps\n",
        ")\n",
        "\n",
        "print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9vudogVBVEm"
      },
      "source": [
        "### Control Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jcsDlRibBXSk",
        "outputId": "663f64b9-70b9-4bce-f88b-bb55d67b4423"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 10):\n",
        "\n",
        "    perceptual_steps = list(range(0, threshold_timestep, (threshold_timestep - 0) // i))\n",
        "    semantic_steps = list(range(threshold_timestep, TIME_STEPS, (TIME_STEPS - threshold_timestep) // (10 - i)))\n",
        "    total_steps = perceptual_steps + semantic_steps\n",
        "\n",
        "    print(f\"perceptual steps : {i}, semantic steps : {10 - i}\")\n",
        "    print(total_steps)\n",
        "\n",
        "    test_denoised_image = model.decoder.DDIM_sampling(\n",
        "        test_noise,\n",
        "        TIME_STEPS,\n",
        "        c=condition,\n",
        "        w=1,\n",
        "        custom_sampling_steps=total_steps\n",
        "    )\n",
        "\n",
        "    print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHu0L3sTDSco"
      },
      "source": [
        "### Evaluation Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIo55MlvDZf1"
      },
      "outputs": [],
      "source": [
        "result = model.evaluate(\n",
        "    epochs = 100,\n",
        "    sampling_type = \"DDIM\",\n",
        "    sampling_time_step = 10,\n",
        "    w = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "B3uuyoiMD9A9",
        "outputId": "a201c6ee-c761-4a86-f038-89cba4c31c34"
      },
      "outputs": [],
      "source": [
        "print_result(result[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04cSyeJPDqWs",
        "outputId": "bad3bb5a-84d4-45e9-d067-e076bc61894c"
      },
      "outputs": [],
      "source": [
        "print_scores(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCqXxYKNEf19"
      },
      "outputs": [],
      "source": [
        "result = model.evaluate(\n",
        "    epochs = 100,\n",
        "    sampling_type = \"DDIM\",\n",
        "    custom_sampling_steps = [0, 54, 108, 162, 216, 270, 416, 562, 708, 854],\n",
        "    w = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r2GsD5c_EqYs"
      },
      "outputs": [],
      "source": [
        "print_result(result[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kjctK_BEsMZ",
        "outputId": "3568882c-af28-4648-b20e-a6f5ea77fed0"
      },
      "outputs": [],
      "source": [
        "print_scores(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj7BrE41-6ff"
      },
      "source": [
        "### Additional Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnQhOikcxE8C",
        "outputId": "56c4228c-cd39-40e4-de7d-ae4d3a83ad31"
      },
      "outputs": [],
      "source": [
        "perceptual_steps = list(range(final_timestep, threshold_timestep, (threshold_timestep - final_timestep) // 5))\n",
        "semantic_steps = list(range(threshold_timestep, TIME_STEPS, (TIME_STEPS - threshold_timestep) // 5))\n",
        "total_steps = perceptual_steps + semantic_steps\n",
        "\n",
        "print(total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "jEflNL7j-8iR",
        "outputId": "816f2b10-b7a3-448c-f12f-3f37d7755c99"
      },
      "outputs": [],
      "source": [
        "test_denoised_image = model.decoder.DDIM_sampling(\n",
        "    test_noise,\n",
        "    TIME_STEPS,\n",
        "    c=condition,\n",
        "    w=1,\n",
        "    custom_sampling_steps=total_steps\n",
        ")\n",
        "\n",
        "print_digits(test_denoised_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CG89WAd_Byo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01d533e252c64d3dae9376ad6e6fb936": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0385b7c88dc14e82b3e6d76a7f659825": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "049faf60564f40929cf10837a85cb182": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c4c3e7bbf7646b6bc4be3991d2ec202": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_badb5db711a34d8a9c3a8810411ee0ff",
            "placeholder": "​",
            "style": "IPY_MODEL_2f4445e401fb4840ba1f92cbc8fdb728",
            "value": " 343M/343M [00:06&lt;00:00, 59.3MB/s]"
          }
        },
        "0ef22d12d8354aadaefaafb86b7b9d43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20c759ad9b1b458abfdf22de18420615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0385b7c88dc14e82b3e6d76a7f659825",
            "max": 228,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb655cdd24d8456c831a96bd3d94da54",
            "value": 228
          }
        },
        "2c437b81dfc24af5be2ca5ac40eb0468": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca9142d5076401a84d8f872373102b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f4445e401fb4840ba1f92cbc8fdb728": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33cc90106bc64d4798c0af552ddef021": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_049faf60564f40929cf10837a85cb182",
            "max": 343291569,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96fc1566c68d4083bbe5a0617e9c62ac",
            "value": 343291569
          }
        },
        "451b946f1dc646d39eeb85eec70b8c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca1719ce3f3c4ef696bfb515baf9cf8b",
            "placeholder": "​",
            "style": "IPY_MODEL_694458ad026f48db8a444fceccf0130e",
            "value": " 985/985 [00:00&lt;00:00, 22.8kB/s]"
          }
        },
        "46f15057baa749e78f82df133d09c51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55eb402489a1457e83c051d47d202fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e1a901b81024c26af435e66ffbad1c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca4dfb81261c481c91d7e8b5cbddb451",
              "IPY_MODEL_33cc90106bc64d4798c0af552ddef021",
              "IPY_MODEL_0c4c3e7bbf7646b6bc4be3991d2ec202"
            ],
            "layout": "IPY_MODEL_97250d9e46d04e1fafce6e4b204e10fa"
          }
        },
        "661c07c474a74f8b8dcb1427b3d2d5ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "694458ad026f48db8a444fceccf0130e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c6e740abd2f416fa4849f714928470f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96fc1566c68d4083bbe5a0617e9c62ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97250d9e46d04e1fafce6e4b204e10fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f246808a43c4ec2922951c51c862d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7affe00cdd94a6e98c9b1e235249cc2",
            "max": 985,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3b50e9ac853462ea4adab3856bcc368",
            "value": 985
          }
        },
        "a03f27a94d6641bdb61f1d25ef1d93ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ef22d12d8354aadaefaafb86b7b9d43",
            "placeholder": "​",
            "style": "IPY_MODEL_2ca9142d5076401a84d8f872373102b1",
            "value": " 228/228 [00:00&lt;00:00, 12.9kB/s]"
          }
        },
        "a138309d763845b091bed4fee9d8356e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa03ed662100470f885d59a9233014ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b49f7eb5071a4254bb915f982ea7a6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5e041e3efe54f96b5567536fa98b0af",
              "IPY_MODEL_20c759ad9b1b458abfdf22de18420615",
              "IPY_MODEL_a03f27a94d6641bdb61f1d25ef1d93ca"
            ],
            "layout": "IPY_MODEL_661c07c474a74f8b8dcb1427b3d2d5ab"
          }
        },
        "badb5db711a34d8a9c3a8810411ee0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b50e9ac853462ea4adab3856bcc368": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5e041e3efe54f96b5567536fa98b0af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01d533e252c64d3dae9376ad6e6fb936",
            "placeholder": "​",
            "style": "IPY_MODEL_46f15057baa749e78f82df133d09c51e",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "c8513cf8f07a44dd8af93b2db38683e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb99d71f3c9b4146a302f8c8ed6d2454",
              "IPY_MODEL_9f246808a43c4ec2922951c51c862d37",
              "IPY_MODEL_451b946f1dc646d39eeb85eec70b8c87"
            ],
            "layout": "IPY_MODEL_a138309d763845b091bed4fee9d8356e"
          }
        },
        "ca1719ce3f3c4ef696bfb515baf9cf8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca4dfb81261c481c91d7e8b5cbddb451": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c6e740abd2f416fa4849f714928470f",
            "placeholder": "​",
            "style": "IPY_MODEL_aa03ed662100470f885d59a9233014ff",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "e7affe00cdd94a6e98c9b1e235249cc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb655cdd24d8456c831a96bd3d94da54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb99d71f3c9b4146a302f8c8ed6d2454": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c437b81dfc24af5be2ca5ac40eb0468",
            "placeholder": "​",
            "style": "IPY_MODEL_55eb402489a1457e83c051d47d202fc9",
            "value": "config.json: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
